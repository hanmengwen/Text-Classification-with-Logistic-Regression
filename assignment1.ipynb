{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM4513-6513] Assignment 1: Text Classification with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test two text classification systems: \n",
    "\n",
    "- **Task 1:** sentiment analysis, in particular to predict the sentiment of movie review, i.e. positive or negative (binary classification).\n",
    "- **Task 2:** topic classification, to predict whether a news article is about International issues, Sports or Business (multiclass classification).\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using (1) unigrams, bigrams and trigrams to obtain vector representations of documents. Two vector weighting schemes should be tested: (1) raw frequencies (**3 marks; 1 for each ngram type**); (2) tf.idf (**1 marks**). \n",
    "- Binary Logistic Regression classifiers that will be able to accurately classify movie reviews trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 1. \n",
    "- Multiclass Logistic Regression classifiers that will be able to accurately classify news articles trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 2. \n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function for Task 1 (**3 marks**)\n",
    "    - Minimise the Categorical Cross-entropy loss function for Task 2 (**3 marks**)\n",
    "    - Use L2 regularisation (both tasks) (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous validation loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength)?  (**2 marks; 0.5 for each model in each task**).\n",
    "- After training the LR models, plot the learning process (i.e. training and validation loss in each epoch) using a line plot (**1 mark; 0.5 for both BOW-count and BOW-tfidf LR models in each task**) and discuss if your model overfits/underfits/is about right.\n",
    "- Model interpretability by showing the most important features for each class (i.e. most positive/negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!).  If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks; 0.5 for BOW-count and BOW-tfidf LR models respectively in each task**)\n",
    "\n",
    "\n",
    "### Data - Task 1 \n",
    "\n",
    "The data you will use for Task 1 are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "### Data - Task 2\n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc..\n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows (not recommended) to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Fri, 20 Mar 2020** and it needs to be submitted via MOLE. Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:31:36.292691Z",
     "start_time": "2020-02-15T14:31:35.549108Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the training, development and test sets \n",
    "training = pd.read_csv('data_sentiment/train.csv',header=-1)\n",
    "development = pd.read_csv('data_sentiment/dev.csv',header=-1)\n",
    "test_sets = pd.read_csv('data_sentiment/test.csv',header=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "# put the raw texts into Python lists\n",
    "train_list=training.values.tolist()\n",
    "development_list=development.values.tolist()\n",
    "test_sets_list=test_sets.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disorder\n",
    "\n",
    "random.shuffle(train_list)\n",
    "random.shuffle(development_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Representation \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they' 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words=[], vocab=set()):\n",
    "    a=re.findall(token_pattern,x_raw)\n",
    "    resultlist=[]\n",
    "    b=[]\n",
    "    #remove stoplist\n",
    "    for i in a:\n",
    "        if i not in stop_words:\n",
    "            b.append(i)\n",
    "    \n",
    "    for i in b:\n",
    "        resultlist.append(i)\n",
    "    \n",
    "   \n",
    "    if ngram_range==(1,2):\n",
    "        i=0\n",
    "        for i in range(0,len(b)-1):\n",
    "            tuble=(b[i],b[i+1])\n",
    "            resultlist.append(tuble)\n",
    "            \n",
    "    if ngram_range==(1,3):\n",
    "        i=0\n",
    "        for i in range(0,len(b)-1):\n",
    "            tuble=(b[i],b[i+1])\n",
    "            \n",
    "            resultlist.append(tuble)  \n",
    "        j=0\n",
    "        for j in range(0,len(b)-2):\n",
    "            tuble=(b[j],b[j+1],b[j+2])\n",
    "            resultlist.append(tuble)\n",
    "    if vocab==set():    \n",
    "        return resultlist  \n",
    "    else:\n",
    "        resultlist2=[]\n",
    "        for i in resultlist:\n",
    "            if i in vocab:\n",
    "                resultlist2.append(i)\n",
    "        return resultlist2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'movie',\n",
       " 'watch',\n",
       " ('great', 'movie'),\n",
       " ('movie', 'watch'),\n",
       " ('great', 'movie', 'watch')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extract_ngrams(\"this is a great movie to watch \", \n",
    "               ngram_range=(1,3), \n",
    "               stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:35.821240Z",
     "start_time": "2020-02-15T14:17:35.814722Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
    "    \n",
    "    \n",
    "    # fill in your code...\n",
    "    #Calculate NGram in all filesï¼ŒNo repetitive NGram removed\n",
    "    all_ngrams=[]\n",
    "    ngrams_list=[]\n",
    "    for s in X_raw:\n",
    "        ngrams_one=extract_ngrams(s, ngram_range=ngram_range,stop_words=stop_words)\n",
    "        all_ngrams.extend(ngrams_one)\n",
    "        ngrams_list.append(ngrams_one)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #get  ngram_counts\n",
    "    Counter_ngrams=Counter(all_ngrams) #Total frequency of each NGram\n",
    "    ngram_counts=Counter_ngrams.most_common(keep_topN)   \n",
    "    #gey  vocab\n",
    "    vocab=[]\n",
    "    for i in range (0,keep_topN):\n",
    "        vocab.append(ngram_counts[i][0])\n",
    "        \n",
    "    #  Disorganize vocab\n",
    "    vocab=list(set(vocab))\n",
    "        \n",
    "        \n",
    "\n",
    "    df={}\n",
    "    for i in vocab:\n",
    "        count=min_df\n",
    "        \n",
    "        for j in range(0,len(ngrams_list)):#  Loop through all files\n",
    "            \n",
    "            if i in ngrams_list[j]:     #If there is NGram in this file\n",
    "                count=count+1\n",
    "        df[i]=count\n",
    "        \n",
    "        \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rawX(data):\n",
    "    rawlist=[]\n",
    "    for i in range(0,len(data)):\n",
    "        rawlist.append(data[i][0])\n",
    "        #print(data[i][0])\n",
    "    return rawlist#  a list of strings each corresponding to the raw text of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_ngram=[]\n",
    "for i in range(0,len(train_list)): \n",
    "    a=extract_ngrams(train_list[i][0], ngram_range=(1,3), stop_words=stop_words)\n",
    "    train_list_ngram.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_raw=get_rawX(train_list)\n",
    "vocab_train, df_train, ngram_counts_train = get_vocab(X_tr_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocabulary id -> word and id -> word\n",
    "\n",
    "vocabulary={}\n",
    "t=0\n",
    "for w in vocab_train:\n",
    "    vocabulary[t]=w\n",
    "    t=t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.213253Z",
     "start_time": "2020-02-15T14:17:39.329147Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "train_list_ngram=[]\n",
    "for i in range(0,len(train_list)): \n",
    "    a=extract_ngrams(train_list[i][0], ngram_range=(1,3), stop_words=stop_words)\n",
    "    train_list_ngram.append(a)\n",
    "development_ngram=[]\n",
    "for i in range(0,len(development_list)): \n",
    "    a=extract_ngrams(development_list[i][0], ngram_range=(1,3), stop_words=stop_words)\n",
    "    development_ngram.append(a)\n",
    "test_ngram=[]\n",
    "for i in range(0,len(test_sets_list)): \n",
    "    a=extract_ngrams(test_sets_list[i][0], ngram_range=(1,3), stop_words=stop_words)\n",
    "    test_ngram.append(a)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    X_vec_list=[]\n",
    "    for i in range(0,len(X_ngram)):  #Loop through each file\n",
    "        a=[]\n",
    "        c = Counter(X_ngram[i])\n",
    "       # print(i)\n",
    "       \n",
    "        for j in range(0,len(vocab)):  #Cycle through each word\n",
    "            \n",
    "            if vocab[j] in X_ngram[i]:\n",
    "                a.append(c[vocab[j]])\n",
    "            else:\n",
    "                \n",
    "                a.append(0) \n",
    "            \n",
    "        X_vec_list.append(a)\n",
    "        \n",
    "        \n",
    "    X_vec=np.asarray(X_vec_list)\n",
    "    \n",
    "\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use vectorise to obtain document vectors for each document in the train, development and test set. \n",
    "def get_rawX(data):\n",
    "    rawlist=[]\n",
    "    for i in range(0,len(data)):\n",
    "        rawlist.append(data[i][0])\n",
    "    return rawlist#  a list of strings each corresponding to the raw text of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_rawX(data):\n",
    "    rawlist=[]\n",
    "    for i in range(0,len(data)):\n",
    "        rawlist.append(data[i][0])\n",
    "        #print(data[i][0])\n",
    "    return rawlist#  a list of strings each corresponding to the raw text of a document\n",
    "\n",
    "\n",
    "##     train\n",
    "X_tr_raw=get_rawX(train_list)\n",
    "vocab_train, df_train, ngram_counts_train = get_vocab(X_tr_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "##   dev\n",
    "X_dev_raw=get_rawX(development_list)\n",
    "vocab_dev, df_dev, ngram_counts_dev = get_vocab(X_dev_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "##   test\n",
    "X_test_raw=get_rawX(test_sets_list)\n",
    "vocab_test, df_test, ngram_counts_test = get_vocab(X_test_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary={}\n",
    "t=0\n",
    "for w in vocab_train:\n",
    "    vocabulary[t]=w\n",
    "    t=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_vector=vectorise(train_list_ngram, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vector=vectorise(development_ngram, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector[:2,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "import math\n",
    "def vectorise_tfidf(X_ngram, vocab,df):\n",
    "    X_vec_list=[]\n",
    "    N=len(X_ngram)\n",
    "    for i in range(0,len(X_ngram)):  #Loop through each file\n",
    "        a=[]\n",
    "        c = Counter(X_ngram[i])\n",
    "        for j in range(0,len(vocab)):  #ycle through each word\n",
    "            \n",
    "            \n",
    "            if (vocab[j] in X_ngram[i])&(vocab[j] in df.keys()):\n",
    "                tf=c[vocab[j]]              \n",
    "                idf=math.log10(N/df[vocab[j]])    \n",
    "                a.append(tf*idf)\n",
    "            else:\n",
    "                a.append(0) \n",
    "            \n",
    "        X_vec_list.append(a)\n",
    "    X_vec=np.asarray(X_vec_list)\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.802265Z",
     "start_time": "2020-02-15T14:17:42.752448Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform your count vectors to tf.idf vectors\n",
    "train_vector_tfidf=vectorise_tfidf(train_list_ngram, vocab_train,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vector_tfidf=vectorise_tfidf(development_ngram, vocab_train,df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_tfidf=vectorise_tfidf(test_ngram, vocab_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:43.211619Z",
     "start_time": "2020-02-15T14:17:43.207266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77187234, 0.09658058, 0.57865785, 0.        , 0.35893698,\n",
       "       0.90330287, 0.58299774, 0.88311318, 0.87162991, 0.51902239,\n",
       "       0.65304381, 0.89175457, 1.26667844, 0.        , 0.        ,\n",
       "       0.74419883, 0.        , 0.79882407, 0.83970769, 2.86730597,\n",
       "       0.        , 0.        , 0.50489089, 1.28576258, 0.94229428,\n",
       "       0.52884413, 0.99563077, 1.20348143, 2.45417652, 0.590075  ,\n",
       "       0.55711891, 3.16765735, 0.61750977, 0.60043778, 0.62149119,\n",
       "       0.        , 0.        , 0.59394847, 0.        , 0.        ,\n",
       "       0.70608769, 0.        , 0.79375709, 1.3663937 , 0.68178342,\n",
       "       0.        , 1.5499544 , 2.17524068, 0.        , 0.74594537])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector_tfidf[1,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # fill in your code...\n",
    "    sig=1/(1 + np.exp(-z))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.351292Z",
     "start_time": "2020-02-15T14:17:44.346822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.00669285 0.76852478]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0)) \n",
    "print(sigmoid(np.array([-5., 1.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    # fill in your code...\n",
    "    # x:1400*5000  w:5000*1\n",
    "    \n",
    "    z=np.dot(X,weights)   \n",
    "    \n",
    "    preds_proba=sigmoid(z)  \n",
    "   \n",
    "    \n",
    "    return preds_proba  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    # fill in your code...\n",
    "    preds_class=[]\n",
    "    preds_proba=np.dot(X,weights)\n",
    "    for i in range(0,len(preds_proba)):\n",
    "        if (preds_proba[i]<0.5):\n",
    "            preds_class.append(0)\n",
    "        else:\n",
    "            preds_class.append(1)\n",
    "            \n",
    "    \n",
    "    return preds_class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def binary_loss(X, Y, weights, alpha=0.00001):  \n",
    "   \n",
    "   # preds_class=predict_class(X, weights)\n",
    "   # l_list=[]\n",
    "    preds_proba_list=predict_proba(X, weights)\n",
    "    preds_proba_=np.asarray(preds_proba_list)\n",
    "    \n",
    "    weights=np.asarray(weights)\n",
    "  \n",
    "\n",
    "    l_=-np.dot(Y.T,np.log(preds_proba_))- np.dot((1-Y).T, np.log(1 - preds_proba_))+alpha*np.dot(weights.T ,weights)\n",
    "    \n",
    "\n",
    "    l=l_/X.shape[0]\n",
    "    return l\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], loss=\"binary\", lr=0.0001, alpha=0.00001, epochs=5, tolerance=0.000001, print_progress=True):\n",
    "    \n",
    "    cur_loss_tr = 1.\n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    \n",
    "    weights=[0 for i in range(5000)]# Initialize weights of 0\n",
    "    weights=np.asarray(weights)\n",
    "    weights= weights.reshape(weights.shape[0],1)\n",
    "    \n",
    "    for i in range(0,epochs):\n",
    "        #Epochs iterations at most (exit the loop if meeting the condition of tolerance = 0.0001)     \n",
    "\n",
    "        for x,y in zip(X_tr,Y_tr):\n",
    "            x=x.reshape(1,x.shape[0])#  1*5000\n",
    "            \n",
    "            \n",
    "            update=np.dot(x.T,(predict_proba(x, weights)-y))\n",
    "            weights=weights-lr*update-2*weights*alpha*lr  \n",
    "        \n",
    "        #get loss\n",
    "        train_loss=binary_loss(X_tr,Y_tr, weights, alpha=0.00001)\n",
    "        \n",
    "        validation_loss=binary_loss(X_dev, Y_dev, weights, alpha=0.00001)\n",
    "        \n",
    "        print('epochs:',i,'trainingloss:',train_loss,'validation_loss:',validation_loss)\n",
    "        training_loss_history.append(train_loss[0])\n",
    "        validation_loss_history.append(validation_loss[0])\n",
    " \n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_tr=[]\n",
    "for i in range(0,len(train_list)):\n",
    "    Y_tr.append(train_list[i][1])\n",
    "    \n",
    "Y_tr=np.asarray(Y_tr)\n",
    "\n",
    "Y_tr=Y_tr.reshape(Y_tr.shape[0],1)\n",
    "\n",
    "\n",
    "Y_dev=[]\n",
    "for i in range(0,len(development_list)):\n",
    "    Y_dev.append(development_list[i][1])\n",
    "Y_dev=np.asarray(Y_dev)\n",
    "\n",
    "Y_dev=Y_dev.reshape(Y_dev.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 trainingloss: [[0.62808878]] validation_loss: [[0.64619578]]\n",
      "epochs: 1 trainingloss: [[0.58369898]] validation_loss: [[0.61545298]]\n",
      "epochs: 2 trainingloss: [[0.54997364]] validation_loss: [[0.59279518]]\n",
      "epochs: 3 trainingloss: [[0.52282221]] validation_loss: [[0.57497822]]\n",
      "epochs: 4 trainingloss: [[0.50015791]] validation_loss: [[0.56036899]]\n",
      "epochs: 5 trainingloss: [[0.48075251]] validation_loss: [[0.54803467]]\n",
      "epochs: 6 trainingloss: [[0.46381562]] validation_loss: [[0.53739434]]\n",
      "epochs: 7 trainingloss: [[0.44880836]] validation_loss: [[0.52806321]]\n",
      "epochs: 8 trainingloss: [[0.43534796]] validation_loss: [[0.51977411]]\n",
      "epochs: 9 trainingloss: [[0.42315367]] validation_loss: [[0.51233443]]\n",
      "epochs: 10 trainingloss: [[0.41201409]] validation_loss: [[0.50560084]]\n",
      "epochs: 11 trainingloss: [[0.4017664]] validation_loss: [[0.4994639]]\n",
      "epochs: 12 trainingloss: [[0.39228272]] validation_loss: [[0.49383805]]\n",
      "epochs: 13 trainingloss: [[0.38346088]] validation_loss: [[0.48865512]]\n",
      "epochs: 14 trainingloss: [[0.37521798]] validation_loss: [[0.48385985]]\n",
      "epochs: 15 trainingloss: [[0.36748588]] validation_loss: [[0.47940674]]\n",
      "epochs: 16 trainingloss: [[0.36020784]] validation_loss: [[0.47525789]]\n",
      "epochs: 17 trainingloss: [[0.35333616]] validation_loss: [[0.47138131]]\n",
      "epochs: 18 trainingloss: [[0.34683031]] validation_loss: [[0.46774976]]\n",
      "epochs: 19 trainingloss: [[0.34065558]] validation_loss: [[0.46433981]]\n",
      "epochs: 20 trainingloss: [[0.33478198]] validation_loss: [[0.46113116]]\n",
      "epochs: 21 trainingloss: [[0.32918345]] validation_loss: [[0.45810611]]\n",
      "epochs: 22 trainingloss: [[0.32383717]] validation_loss: [[0.4552491]]\n",
      "epochs: 23 trainingloss: [[0.31872304]] validation_loss: [[0.45254637]]\n",
      "epochs: 24 trainingloss: [[0.31382327]] validation_loss: [[0.44998574]]\n",
      "epochs: 25 trainingloss: [[0.30912205]] validation_loss: [[0.44755632]]\n",
      "epochs: 26 trainingloss: [[0.30460524]] validation_loss: [[0.44524836]]\n",
      "epochs: 27 trainingloss: [[0.30026016]] validation_loss: [[0.4430531]]\n",
      "epochs: 28 trainingloss: [[0.29607537]] validation_loss: [[0.44096264]]\n",
      "epochs: 29 trainingloss: [[0.29204054]] validation_loss: [[0.43896982]]\n",
      "epochs: 30 trainingloss: [[0.2881463]] validation_loss: [[0.43706814]]\n",
      "epochs: 31 trainingloss: [[0.28438412]] validation_loss: [[0.43525168]]\n",
      "epochs: 32 trainingloss: [[0.28074623]] validation_loss: [[0.43351505]]\n",
      "epochs: 33 trainingloss: [[0.27722549]] validation_loss: [[0.4318533]]\n",
      "epochs: 34 trainingloss: [[0.27381538]] validation_loss: [[0.43026191]]\n",
      "epochs: 35 trainingloss: [[0.27050988]] validation_loss: [[0.42873671]]\n",
      "epochs: 36 trainingloss: [[0.26730347]] validation_loss: [[0.42727387]]\n",
      "epochs: 37 trainingloss: [[0.26419102]] validation_loss: [[0.42586985]]\n",
      "epochs: 38 trainingloss: [[0.2611678]] validation_loss: [[0.42452139]]\n",
      "epochs: 39 trainingloss: [[0.25822941]] validation_loss: [[0.42322545]]\n",
      "epochs: 40 trainingloss: [[0.25537179]] validation_loss: [[0.42197924]]\n",
      "epochs: 41 trainingloss: [[0.25259113]] validation_loss: [[0.42078015]]\n",
      "epochs: 42 trainingloss: [[0.24988389]] validation_loss: [[0.41962575]]\n",
      "epochs: 43 trainingloss: [[0.24724677]] validation_loss: [[0.41851379]]\n",
      "epochs: 44 trainingloss: [[0.24467668]] validation_loss: [[0.41744215]]\n",
      "epochs: 45 trainingloss: [[0.24217072]] validation_loss: [[0.41640888]]\n",
      "epochs: 46 trainingloss: [[0.23972618]] validation_loss: [[0.41541214]]\n",
      "epochs: 47 trainingloss: [[0.2373405]] validation_loss: [[0.4144502]]\n",
      "epochs: 48 trainingloss: [[0.2350113]] validation_loss: [[0.41352145]]\n",
      "epochs: 49 trainingloss: [[0.23273632]] validation_loss: [[0.41262437]]\n",
      "epochs: 50 trainingloss: [[0.23051343]] validation_loss: [[0.41175754]]\n",
      "epochs: 51 trainingloss: [[0.22834062]] validation_loss: [[0.41091963]]\n",
      "epochs: 52 trainingloss: [[0.22621599]] validation_loss: [[0.41010937]]\n",
      "epochs: 53 trainingloss: [[0.22413776]] validation_loss: [[0.40932558]]\n",
      "epochs: 54 trainingloss: [[0.22210423]] validation_loss: [[0.40856714]]\n",
      "epochs: 55 trainingloss: [[0.2201138]] validation_loss: [[0.40783298]]\n",
      "epochs: 56 trainingloss: [[0.21816492]] validation_loss: [[0.40712212]]\n",
      "epochs: 57 trainingloss: [[0.21625617]] validation_loss: [[0.4064336]]\n",
      "epochs: 58 trainingloss: [[0.21438616]] validation_loss: [[0.40576653]]\n",
      "epochs: 59 trainingloss: [[0.21255359]] validation_loss: [[0.40512007]]\n",
      "epochs: 60 trainingloss: [[0.21075722]] validation_loss: [[0.40449341]]\n",
      "epochs: 61 trainingloss: [[0.20899585]] validation_loss: [[0.40388578]]\n",
      "epochs: 62 trainingloss: [[0.20726836]] validation_loss: [[0.40329647]]\n",
      "epochs: 63 trainingloss: [[0.20557368]] validation_loss: [[0.40272479]]\n",
      "epochs: 64 trainingloss: [[0.20391077]] validation_loss: [[0.40217007]]\n",
      "epochs: 65 trainingloss: [[0.20227865]] validation_loss: [[0.40163171]]\n",
      "epochs: 66 trainingloss: [[0.20067638]] validation_loss: [[0.40110911]]\n",
      "epochs: 67 trainingloss: [[0.19910307]] validation_loss: [[0.40060169]]\n",
      "epochs: 68 trainingloss: [[0.19755785]] validation_loss: [[0.40010894]]\n",
      "epochs: 69 trainingloss: [[0.19603991]] validation_loss: [[0.39963032]]\n",
      "epochs: 70 trainingloss: [[0.19454845]] validation_loss: [[0.39916535]]\n",
      "epochs: 71 trainingloss: [[0.19308272]] validation_loss: [[0.39871356]]\n",
      "epochs: 72 trainingloss: [[0.191642]] validation_loss: [[0.39827451]]\n",
      "epochs: 73 trainingloss: [[0.19022558]] validation_loss: [[0.39784776]]\n",
      "epochs: 74 trainingloss: [[0.18883279]] validation_loss: [[0.39743291]]\n",
      "epochs: 75 trainingloss: [[0.187463]] validation_loss: [[0.39702956]]\n",
      "epochs: 76 trainingloss: [[0.18611559]] validation_loss: [[0.39663735]]\n",
      "epochs: 77 trainingloss: [[0.18478997]] validation_loss: [[0.3962559]]\n",
      "epochs: 78 trainingloss: [[0.18348555]] validation_loss: [[0.39588488]]\n",
      "epochs: 79 trainingloss: [[0.1822018]] validation_loss: [[0.39552396]]\n",
      "epochs: 80 trainingloss: [[0.18093817]] validation_loss: [[0.39517281]]\n",
      "epochs: 81 trainingloss: [[0.17969417]] validation_loss: [[0.39483113]]\n",
      "epochs: 82 trainingloss: [[0.17846929]] validation_loss: [[0.39449864]]\n",
      "epochs: 83 trainingloss: [[0.17726306]] validation_loss: [[0.39417505]]\n",
      "epochs: 84 trainingloss: [[0.17607503]] validation_loss: [[0.39386008]]\n",
      "epochs: 85 trainingloss: [[0.17490475]] validation_loss: [[0.39355349]]\n",
      "epochs: 86 trainingloss: [[0.17375179]] validation_loss: [[0.39325502]]\n",
      "epochs: 87 trainingloss: [[0.17261574]] validation_loss: [[0.39296444]]\n",
      "epochs: 88 trainingloss: [[0.17149621]] validation_loss: [[0.3926815]]\n",
      "epochs: 89 trainingloss: [[0.1703928]] validation_loss: [[0.39240599]]\n",
      "epochs: 90 trainingloss: [[0.16930514]] validation_loss: [[0.3921377]]\n",
      "epochs: 91 trainingloss: [[0.16823288]] validation_loss: [[0.39187641]]\n",
      "epochs: 92 trainingloss: [[0.16717566]] validation_loss: [[0.39162193]]\n",
      "epochs: 93 trainingloss: [[0.16613314]] validation_loss: [[0.39137407]]\n",
      "epochs: 94 trainingloss: [[0.16510501]] validation_loss: [[0.39113264]]\n",
      "epochs: 95 trainingloss: [[0.16409093]] validation_loss: [[0.39089747]]\n",
      "epochs: 96 trainingloss: [[0.16309061]] validation_loss: [[0.39066837]]\n",
      "epochs: 97 trainingloss: [[0.16210375]] validation_loss: [[0.39044519]]\n",
      "epochs: 98 trainingloss: [[0.16113005]] validation_loss: [[0.39022777]]\n",
      "epochs: 99 trainingloss: [[0.16016924]] validation_loss: [[0.39001594]]\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(train_vector,Y_tr, \n",
    "                                             dev_vector, \n",
    "                                             Y_dev, \n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.598911Z",
     "start_time": "2020-02-15T14:17:51.482307Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x344 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 4.3), dpi=80)\n",
    "plt.title(\"Training Monitoring\")\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loss_tr_count,color='blue',linewidth=3.0)\n",
    "plt.plot(dev_loss_count,color='orange',linewidth=3.0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "Explain here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_te=[]\n",
    "for i in range(0,len(test_sets_list)):\n",
    "     Y_te.append(test_sets_list[i][1])\n",
    "test_vector=vectorise(test_ngram, vocab_train)\n",
    "preds_te_count=predict_class(test_vector,w_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:40.569499Z",
     "start_time": "2020-02-15T14:37:40.566796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n",
      "Precision: 0.8941176470588236\n",
      "Recall: 0.76\n",
      "F1-Score: 0.8216216216216216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function can return top-10 words for the negative and positive class\n",
    "def top_words(w_list):\n",
    "    w_sort_positive=sorted(w_list,reverse = True)\n",
    "    w_positive_index=[]\n",
    "    positive_list=[]\n",
    "    for i in range(10):\n",
    "        index=w_list.index(w_sort_positive[i])\n",
    "        w_positive_index.append(index)\n",
    "        positive_list.append(vocabulary[index])\n",
    "        \n",
    "    w_sort_negative=sorted(w_list)\n",
    "    w_negative_index=[]\n",
    "    negative_list=[]\n",
    "    for i in range(10):\n",
    "        index=w_list.index(w_sort_negative[i])\n",
    "        w_negative_index.append(index)\n",
    "        negative_list.append(vocabulary[index])    \n",
    "    return positive_list,negative_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:07.638179Z",
     "start_time": "2020-02-15T14:37:07.635838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_list: ['great', 'well', 'seen', 'fun', 'also', 'many', 'both', 'movies', 'life', 'see']\n",
      "negative_list: ['bad', 'unfortunately', 'only', 'script', 'worst', 'boring', 'why', 'plot', 'any', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "#print the top-10 words for the negative and positive class respectively\n",
    "positive_list,negative_list=top_words(w_count.tolist())\n",
    "print('positive_list:',positive_list)\n",
    "print('negative_list:',negative_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 trainingloss: [[0.63885208]] validation_loss: [[0.66621195]]\n",
      "epochs: 1 trainingloss: [[0.59679669]] validation_loss: [[0.64532873]]\n",
      "epochs: 2 trainingloss: [[0.56221545]] validation_loss: [[0.62801918]]\n",
      "epochs: 3 trainingloss: [[0.53301369]] validation_loss: [[0.61330603]]\n",
      "epochs: 4 trainingloss: [[0.50786392]] validation_loss: [[0.6005835]]\n",
      "epochs: 5 trainingloss: [[0.48586247]] validation_loss: [[0.58942992]]\n",
      "epochs: 6 trainingloss: [[0.46636867]] validation_loss: [[0.57953825]]\n",
      "epochs: 7 trainingloss: [[0.44891362]] validation_loss: [[0.57067882]]\n",
      "epochs: 8 trainingloss: [[0.43314492]] validation_loss: [[0.56267628]]\n",
      "epochs: 9 trainingloss: [[0.41879175]] validation_loss: [[0.55539438]]\n",
      "epochs: 10 trainingloss: [[0.40564196]] validation_loss: [[0.54872561]]\n",
      "epochs: 11 trainingloss: [[0.39352655]] validation_loss: [[0.54258389]]\n",
      "epochs: 12 trainingloss: [[0.38230892]] validation_loss: [[0.53689949]]\n",
      "epochs: 13 trainingloss: [[0.37187723]] validation_loss: [[0.5316152]]\n",
      "epochs: 14 trainingloss: [[0.36213886]] validation_loss: [[0.52668364]]\n",
      "epochs: 15 trainingloss: [[0.35301629]] validation_loss: [[0.52206517]]\n",
      "epochs: 16 trainingloss: [[0.34444407]] validation_loss: [[0.51772634]]\n",
      "epochs: 17 trainingloss: [[0.33636643]] validation_loss: [[0.51363868]]\n",
      "epochs: 18 trainingloss: [[0.32873551]] validation_loss: [[0.5097778]]\n",
      "epochs: 19 trainingloss: [[0.32150995]] validation_loss: [[0.50612265]]\n",
      "epochs: 20 trainingloss: [[0.31465377]] validation_loss: [[0.50265495]]\n",
      "epochs: 21 trainingloss: [[0.30813548]] validation_loss: [[0.49935872]]\n",
      "epochs: 22 trainingloss: [[0.30192738]] validation_loss: [[0.49621993]]\n",
      "epochs: 23 trainingloss: [[0.29600496]] validation_loss: [[0.4932262]]\n",
      "epochs: 24 trainingloss: [[0.29034645]] validation_loss: [[0.49036653]]\n",
      "epochs: 25 trainingloss: [[0.28493243]] validation_loss: [[0.48763114]]\n",
      "epochs: 26 trainingloss: [[0.27974549]] validation_loss: [[0.48501125]]\n",
      "epochs: 27 trainingloss: [[0.27476996]] validation_loss: [[0.482499]]\n",
      "epochs: 28 trainingloss: [[0.26999173]] validation_loss: [[0.48008727]]\n",
      "epochs: 29 trainingloss: [[0.26539797]] validation_loss: [[0.47776966]]\n",
      "epochs: 30 trainingloss: [[0.26097708]] validation_loss: [[0.47554031]]\n",
      "epochs: 31 trainingloss: [[0.25671844]] validation_loss: [[0.47339393]]\n",
      "epochs: 32 trainingloss: [[0.25261239]] validation_loss: [[0.47132565]]\n",
      "epochs: 33 trainingloss: [[0.24865004]] validation_loss: [[0.46933103]]\n",
      "epochs: 34 trainingloss: [[0.24482326]] validation_loss: [[0.467406]]\n",
      "epochs: 35 trainingloss: [[0.24112455]] validation_loss: [[0.4655468]]\n",
      "epochs: 36 trainingloss: [[0.237547]] validation_loss: [[0.46374995]]\n",
      "epochs: 37 trainingloss: [[0.23408421]] validation_loss: [[0.46201226]]\n",
      "epochs: 38 trainingloss: [[0.23073028]] validation_loss: [[0.46033075]]\n",
      "epochs: 39 trainingloss: [[0.2274797]] validation_loss: [[0.45870266]]\n",
      "epochs: 40 trainingloss: [[0.22432738]] validation_loss: [[0.45712544]]\n",
      "epochs: 41 trainingloss: [[0.22126856]] validation_loss: [[0.45559668]]\n",
      "epochs: 42 trainingloss: [[0.21829881]] validation_loss: [[0.45411415]]\n",
      "epochs: 43 trainingloss: [[0.21541399]] validation_loss: [[0.45267577]]\n",
      "epochs: 44 trainingloss: [[0.21261024]] validation_loss: [[0.45127958]]\n",
      "epochs: 45 trainingloss: [[0.20988393]] validation_loss: [[0.44992375]]\n",
      "epochs: 46 trainingloss: [[0.20723166]] validation_loss: [[0.44860655]]\n",
      "epochs: 47 trainingloss: [[0.20465024]] validation_loss: [[0.44732637]]\n",
      "epochs: 48 trainingloss: [[0.20213667]] validation_loss: [[0.44608168]]\n",
      "epochs: 49 trainingloss: [[0.19968812]] validation_loss: [[0.44487103]]\n"
     ]
    }
   ],
   "source": [
    "w_tfidf, trl, devl = SGD(train_vector_tfidf, Y_tr, \n",
    "                         X_dev=dev_vector_tfidf, \n",
    "                         Y_dev=Y_dev, \n",
    "                         lr=0.0001, \n",
    "                         alpha=0.00001, \n",
    "                         epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAFFCAYAAABSX8KfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5hU1f3H8feXBZaysCC9N0FAlI5iQRHsGgtoYo1GjSY/E40x0TSjppvEmMQYe0USTbAiqDSVLk1ERZCy9N4XFthlz++PM5OZXbbvzNyZnc/reeaZuXfu3Pnudd0P595zzzHnHCIiIsmsVtAFiIiIlEdhJSIiSU9hJSIiSU9hJSIiSU9hJSIiSU9hJSIiSU9hJSIiSU9hJWnLzOaY2c8rsf3vzWxyPGtKNDO7zcxWxGnf/zazp+Oxb0k/ppuCJZmZWXm/oMOdcx9Ucd/HAIecc/sruH0WUMc5t6sq31fB7/g9cA/wlHPu28XemwGcClznnBsTo++rDzR0zm0PLf8X2O6cuy0G+84GnHNub3X3JVI76AJEytEm6vU9wEnA5VHrdhb/gJllOucOlbdj59xRny1n+9zKbF8N64ErzOx74Z/DzLoBJwB7YvlFzrk8IC+W+zSzOkCBcy6mtUp602lASWrOuc3hB7AfOBy9zjl3OHx6zszuMbNNwAwAM7vPzJaZ2QEzW25mRVoL0acBzayemTkzu9bMPgh95mMz6x21fZHTgKHP/9rMnjGzXDNbZWaXF/uOG8xsfej9p83sb2b2bjk/9jJgJfC1qHXXA68BB4vtv5eZTTWzg2a22cx+a2a1ot7fbGbfN7M3Qj/T52Z2etT7/zsNGGrVjQJuDR2Lg1Hb3Rz6+Q6Z2WIzGxn13nlmVmBm55vZ0lCNDYufBiyvltA2PzGzbWa2K3S8/2tmj5dzvCQNKKykpjgZ6AOMAK4LrTsAfAs4Hrgf+IuZnVXOfn4J/BHoD+wCnipn+/8DFgH9gFeAF8ysKYCZnQA8A/wFGAhsAG6o4M/zYvjnMDMDrg2t+59QC+btUJ2DgFuAbwN3FtvXT4BXgb7APGCMmZV0VuXXwPjQ97QBOoW+5yzgMfxxORGYCLxtZu2iPlsL+ClwY2ibIqFakVrM7CL88f8RMBRoCpxTyn4kzSispKY4DHzbOfeFc+5LAOfcn5xzM51zq51zY/FhMrqc/fzdOfeOc24Z8HtgqJnVLWP7ac65R51zK/B/aOvhgwl8cExzzv3ZObfMOfdLoKKdGf4FjDCz5sDp+FP2HxTb5kKgFXCjc+4z59zb+MD5QfF9OefGOue+Ah4AOgKdi39h6DTnISAv1GrdEnrr+8C/nXP/DP0c9+Jbf9EtVQPuds7Ncc597pwrKO3nKqOWW4ExzrnnQ/8Nb8f/g0NEYSU1xpeh6y//Y2aXmtksM9tqZrnA1UCHcvazJOr1Zvwf4eYV2d45dxh/Da1laFV3YEGx7eeX8/3hfW0DpgBX4U8BjnFH94Y6DlharAPDbKC9mTUsqUb8z0RUjRVxHDCn2LrZofXRPqnAvsqqpcjxcs7lA59WvEypyRRWUlMU+Re4mfUE/oM/ZXU+kdN0dcrZT37U63A4lPX/SX6xZRe1vUXtoypeAm4Crgi9Ls4quJ/K/kxV+Z4jFenUUk4t1T1eUoMprKSmGgTscM79yjm3IHSarmuCa1hO5JRgWPHlsryFv260LHxqs5gvgV5m1jhq3VBgXUW745cgH8go4XtOLrZuaGh9LBU5XqFrcifG+DskRanrutRUK4EWZnYNMBf4Jr7r96wE1vAk8ImZ/QB4B39KrzuwtSIfds4dMrPOQGnXf8YDW4DnzOw+fBj/HPhtNWpeA5xjZp2AXOfcDuCvwHtmNguYiu+0chzwRDW+pyRPAP8xs+n4/2Z3Ag1Qa0tQy0pqKOfcbOBXwCP46yDNgecSXMMS4GbgbmAh/nrZK/hODBXdx57SWkmhTgwX43vNLQCexgfkI9Uo+5/4+66+xPdexDk3Dfgu8GPgM/xp1Yudcxuq8T1Hcc6NBx4EHsaH1W5gOpU4XlJzaQQLkQQKjUIx1zn3w6BrSXZmlgGsAv7knPt70PVIsHQaUCSOzOz7wIf4+46uBk4B7gi0qCRmZj/Gn940/HFqjr8ZWtKcwkokvvrhryNl4U+tfc05V7w7u0SMBO7F99pcAoyI9elGSU06DSgiIklPHSxERCTpKaxERCTppfw1q8zMTNeiRYugyxARkWrasGHDYedcZknvpXxYtWjRgvXr1wddhoiIVJOZbSvtPZ0GFBGRpKewEhGRpKewEhGRpKewEhGRpKewEhGRpKewEhGRpKewEhGRpKewOrwL9sR6wlMREYml9A6rdW/A+N4w/XI4ovndRESSVfqG1brXYPplcHAz7F0Kn/066IpERKQU6RtW7S6Gpv0iy1/8HnYtDq4eEREpVfqGVa06cPJzYKHhEV0BzPkWFBYEW5eIiBwlfcMKfMuq9z2R5V0LYemfgqtHRERKlN5hBdDnF9C4V2R5yf2wd1lg5YiIyNEUVhmZcNIzgPnlwkMw9yZwhYGWJSIiEQorgBZD4bg7IsvbZsLyfwRXj4iIFKGwCuv7a8jqGlle/BPIzQmsHBERiVBYhdVuCEOeiiwX7IePbwHngqtJREQAhVVRrc+CY78dWd48GVY9F1w9IiICKKyO1u8hqN8usrzwLti/Jrh6REREYXWUutkw5InIcv4e+OhyKMgLriYRkTSX9mF1+DBMn15sZbsLoeuNkeVdC2H+d3X9SkQkIGkbVjk5cN110LIlDBsGa4qf6Rv0j6JjB656HlY8gYiIJF7ahlXDhjB2LOzZ45dfe63YBrXrw+mvQd1jIusWfB+2zU5YjSIi4qVtWLVo4VtUYePGlbBRVhc49V9ERrfIhxmjIW9LIkoUEZGQtA0rgFGjIq9nzYJNm0rYqM050Pc3keW8jTDzSh9cIiKSEGkdVpddFnntHLz+eikb9r4X2l8aWd76ESz6cVxrExGRiLQOq3bt4JRTIsslngoEMIOhL0CjHpF1yx6BnH/FtT4REfHSOqyg6KnADz+E7dtL2bBOYxj2OtTOiqybexNsnRHX+kRERGHF5ZdHXh85Am++WcbG2b3h5OejPpAHH14EuxbHqzwREUFhRefOMHBgZLnUU4FhHUfBCfdHlvP3wLRzYd+KOFQnIiKgsAKKngqcPBl27y7nA33ugx63R5YPboGpZ8OBjXGpT0Qk3SmsKBpW+fkwfnw5HzCDgX+FTldH1u3PgWnnwKGd8ShRRCStKayAHj2gT5/IcrmnAgGsFgx9HtpeGFm353P44ALIz411iSIiaU1hFRLdunr3XcitSN7UqgOnvQotTous2zEXpl8ORw7FvEYRkXSlsAqJDquDB2HixAp+sHYDOONtaNI3sm7zJJh1NRw5HNMaRUTSlcIqpE8f6N49slyhU4FhdZvA8Pcg69jIunWvwUeXQMGBmNUoIpKuFFYhZkVbV++841tYFVa/FZw1qegsw5ve9Z0uDpfXvVBERMqisIoSHVa5ufD++5XcQVZnOHs6ZHWNrNs2EyafqZHaRUSqQWEVZeBA6NQpslypU4FhWV3g7BnQ5ITIut2LYfLpsL/4DI8iIlIRcQ8rM+tuZrPMbLmZfWxmvUvZ7gwzm2dmn5vZl2Y2NN61HV1D0eGX3nrLT3tfafXbwIgPoNnJkXX7voJJp8GeL6tbpohI2klEy+oJ4EnnXA/gIeCZ4huYWVvgBeB659zxQD9gaQJqO0r0qcDdu2HatCruKPMYfw2r9dmRdQfW+xbWzgXVqlFEJN3ENazMrCUwABgTWjUO6GJmnYtt+l1gjHNuKYBz7qBzLpBeCUOHQps2keUqnQoMq5Plu7V3iGquHdoOk06Htf+pxo5FRNJLvFtWHYCNzrkCAOecA9YCHYtt1xuob2aTzewTM/u7mTUoaYdmdpeZrQ8/cit0927F1apVdFLGN97wo7FXWUYmnPoKdL0xsu5IHsy4Ehb/AlxhNXYuIpIeEnEa0BVbthK2qQOcCVwBDAKygftL3JlzDzvn2ocfWVlZJW1WLdGnArdtgw8+qOYOa9WGk56G439adP3nv/ajXeTvq+YXiIjUbPEOq3VAezOrDWBmhm9trS223RrgHefcrlAr7N/AkDjXVqphw6Bly8jyM0ddZasCqwV9fwOnjIWMepH169+E94dC7qoYfImISM0U17Byzm0FFgHXhlaNAnKccznFNh0LDDezzNDyeUBgMxrWrg3XXx9ZHjcOduyI0c47XwUjpxe9eXjP5/DuYNg8NUZfIiJSsyTiNOCtwK1mthy4F7gJwMwmmNkgAOfcLOBt4BMzWwK0AO5LQG2luvnmyOvDh+Gll2K482aD4Lz50Dyqd/7hnX60i6UPgyt+5lREJL2ZS/E/jO3bt3fr16+Py77POAM++si/7t0bPvvM34sVM0cOwbzvwKrniq5vc76ffqReyxI/JiJSE5nZBudc+5Le0wgWZbjllsjrL76A2bNj/AUZmXDSMzDgEX9NK2zTRJhwImyaFOMvFBFJTQqrMowaBU2aRJafeioOX2IGPe+AEdOgQdQ/KA5u8acFF/1YU42ISNpTWJWhfn247rrI8iuvwJ49cfqylsPg/MXQ/rKi65f+ESadCvtWxOmLRUSSn8KqHNGnAvPyYOzYOH5Z5jFw+jgY/HjR7u0758PE/rDiKXW+EJG0pLAqxwknwEknRZbjciowmhl0vxXOnV905PaCXPj42zB1BOxbGeciRESSi8KqAqK7sS9aBAsXJuBLmxwP58yF7v9XdP2WaTDhBN/FvbA640CJiKQOhVUFfOMbED2qU9xbV2G168PgR+HMCdCgQ2T9kTxY9EOYdArs/ixBxYiIBEdhVQFZWXDVVZHll1+G/fsTWEDb8+HCz6D7d4uu3/ExvDsAljzg79kSEamhFFYVFN3RYt8+ePXVBBdQpzEM/geM/BAadY+sL8yHJffDO8fDhvEJLkpEJDEUVhU0aBD07RtZTtipwOLCXdx7/bjojcS5K+HDi+GDC2Hv8oCKExGJD4VVBZkVbV3Nng2ffx5QMbXrQ/8/+A4YTfsXfW/jBJjQBxbdo6lHRKTGUFhVwjXX+BuFwwJrXYU1GwTnzoMhT0Bms8j6wnxY+hCMPw5Wv6QJHkUk5SmsKqFJE7jiisjyiy/CwYPB1QNArQw49ttw0XLocXvRU4N5m2D29fDuINj4nm4oFpGUpbCqpOhTgbt2+cBKCpnHwKC/w3mLoOWZRd/btQg+OM/fULx9biDliYhUh6YIqSTnoH9/WByaGvLYY+HLLyEjI2EllM85WPdfWHg3HCg+KTPQ4XI48TeQ3TPxtYmIlEJThMSQGdxzT2R5xQp47bXg6imRGXS8Ai5eDgP+ApnNi76/7jWYcDzMvRlyVwdTo4hIJahlVQUFBdCjB6wO/Z0fMADmz4/xxIyxlL8Xlv4ZvvwzFBS7m9kyoMt10Pun0Lh7yZ8XEUkAtaxirHZtuPvuyPLChTBlSnD1lKtOYzjxAbh4JfT4HtSqE3nPHYFVz8M7PWHmNbDni8DKFBEpjVpWVZSXB506wbZtfnnECJg8OeFlVE3uKljyIOSM8WFVhEGHUdDn59C0b4kfFxGJB7Ws4qB+fbjjjsjylCn+VGBKyOoKQ5/317S63VK0pUWoc8bEfjD1HHV5F5GkoJZVNezaBR07Qm6uXx49Gv7zn0BKqZ79a+GLh2Dl01BYwoC42X2g1w+h01WQkZn4+kQkLahlFSdNm8Ktt0aWx42D5ak4LF/Djn4qkq+tguN+ABkNir6/5zOYcyO82Rk+/y0c2hlImSKSvtSyqqYNG6BLF8jP98u33AJPPhlYObFxaCeseByW/Q0Objn6/Yx6vpXV4//gmIGJr09EaqSyWlYKqxi46SZ49ln/um5dyMmBNm0CLSk2jhyCnLG+y/ueUkbtbXaSD62OV/gQExGpIp0GjLMf/Shyj9Xhw/DII8HWEzMZmdDtRrhgCZw5EVqPPHqbHXP9+INvdIBP7vU9DUVEYkwtqxgZNSoykkWjRrB2rR/4tsbZ/Tl89RisfhEKckveptVZ0O1m6HCZWlsiUmFqWSVA9BBM+/bB448HV0tcNTnez1h82QYY9Chk9z56my1TYdbV8HpbmP892PVJ4usUkRpFLasYOussmDbNv27ZElatgoYNg60p7pyDrR/C8n/A+jfAFZS8XdMB0PVG6PR1qNcisTWKSEpQyypBoltXW7fCX/8aXC0JYwatzoTT/wOXrod+D0Hj447ebtdCWPA939r64GJY8yoU5CW8XBFJTWpZxZBzcNppMGuWX27cGFauhObNy/5cjeMcbJ8FK5+BNa/AkQMlb1enMXQYDV2uhZZnFJ04UkTSjrquJ9CMGXD66ZHlO+6oQb0DqyJ/nw+s1S/Athmlb1e/re/+3vFKaH6ygkskDSmsEuySS+Ctt/zrOnX85IxduwZbU1LIXQ05L8Pql2BfGUN9NOjgQ6vT1+GYQUk894qIxJLCKsG++AJOOAEKC/3yVVfB2LHB1pRUnIOd831orfk3HNpW+rYNu/iZjTtcBs2HqsUlUoMprAJwyy3w9NOR5QUL/CSNUkxhPmyeCmtfgXWvQ/7u0ret1xraXwLtL4NWwyGjbuLqFJG4U1gFYONGOPZYP+8VwMiRMGlSsDUlvSOHYfMkf41r/RtQsK/0betkQ7uLoN3F0OY8qJuduDpFJC4UVgH52c/gt7+NLL/3HpxzTnD1pJQjB2Hju7DuNdjwdtktLqsNLYf54Gp3MTTqlrg6RSRmFFYB2bMHunWDHTv8cr9+/nRgLV12qZzCfNjygQ+u9W/Awc1lb9+4pw+ttudD81N1ulAkRSisAvTII/CDH0SWX3oJrr02uHpSniuE7XNg/euw/q2yexUC1M6C1iOgzfnQ9jxo2CkxdYpIpSmsAnToEPTs6acNAejUCZYtg0xNuBsbe5fDhvH+VOG26eCOlL19417+Glebs/2pw9o1fTwskdShsArY2LFwzTWR5YcfLtrakhg5vAs2vueDa/N7cGhH2dvXqgPNT4HWZ/vpT44ZBLUyElOriBxFYRWwwkIYPBgWLvTLTZv6G4Vbtgy2rhqt8AjsXACbJvqOGjvmAuX8rtdp4rvEhx/Zx+uGZJEEUlglgcmT4eyzI8vXXw8vvBBcPWnn0A7YNMmH1+ZJkLep/M9ktvCD9LYaDi2H+wF6FV4icRNoWJlZd+AFoDmwG7jBOfdFsW1uAB4BckKrdjnnhldk/6kSVgCjR8O4cZHlqVNheIV+Sokp52DvUh9emyfD1g9Kn0gyWr3W0PJ0aDHMX+9q0kcjaojEUNBhNRV40Tn3vJmNBn7onBtabJsbgIucc6Mru/9UCqsNG3xni9zQ38UePeDTT9XZInBHDsOOj32La8s02DHHd5cvT92m0OI0H1wtTvNzdqmbvEiVBRZWZtYSWA40d84VmJkBm4CTnXM5UdvdQBqEFfg5ru68M7L84IPwi18EV4+UoOAAbJvpg2vLNNg5r/xehgAZ9eCYwdDiVP9oPhQym8W/XpEaIsiwGgi85JzrHbXuY+Bu59xHUetuAP4IbAD2A39xzv23lH3eBdwVXs7Ozm63e3cZoxskmYICOOmkSGeLzExYsgS6dw+2LilD/j4/vcnW6bDtI98Kq0jLC/wNys1P8dOeND8ZGvdWj0ORUgQdVi86546PWjcPfyowOqyaAweccwfMrBfwPnCFc25Oed+Rai0rgHnzfGCFD/3IkfD++7p2nzIK8nzvwq0f+cf22aVPMFlc7UbQbEgkvJoNgXrqFioCwZ8G/ApoVtZpwBI+9wSw3Dn35/K+IxXDCuB734NHH40sjx3rpxKRFFSYD7sW+1OH22f657yNFf98w87QbLAPrmZD/LWvOllxK1ckWQXdweID4PmoDhZ3O+dOLrZNO+fchtDrVsAM4Fbn3NTy9p+qYbVnD/TqBZtCPahbtfL3XjVpEmxdEgPOwf41kfDaPhd2L67YdS/wPQwb94Zmg6DpQP/cpC/Urh/fukUCFnRYHQc8DzQD9gLfdM59bmYTgPucc/PN7LfAJUA+UAt43Dn3WEX2n6phBfDqq/D1r0eWb7sN/vnP4OqROCrY729S3j4n9Jhd/oC80SwDsnv7UTaOGQhN+0PTvhouSmoU3RScpJyDCy6Ad9/1y2YwaxacfHLZn5MawDk4sA52zPMdNnZ87GdPrsj9Xv9j0LhHKLgGwDH9/Wv1QJQUpbBKYqtWwfHHw8GDfrlPH98Bo169YOuSABQegX3LQuE1z7fEdi/2c3tVRv12vtXVpK9/btoPso5VL0RJegqrJPfb3/qJGsPuugv+XG7XEkkLhfmwZ6lvde1c4J93f1r5AMuoD01O8I/sE6Dpif65XvP41C1SBQqrJHf4MAwdGrn3CmDKFDjrrOBqkiRWWAB7v4Rdi2DnIv+8axHk76n8vuq1jgqxPn4Iqca91BtRAqGwSgFLl8KAAZHTge3b+6GYmjYNti5JEc7B/tWw6xPfjX73Yv+8P6dq+2vYxY8636SPf87u7W9wrt0gpmWLRFNYpYhHH/X3X4VddZW//0qkyg7v8acNd30Cu5f413s+870TK838PWHZvSOPxr38aPR1dc+FVJ/CKkU4B+efD++9F1n38stw9dXB1SQ1kCv0La7dS2DXp7BnCez+DPYtr/i9YMXVa+1bXtm9/HPjnj7EGnTQyPRSYQqrFLJxI5xwAuzc6Zezs/3pwI4dg61L0sCRQz6wdn8Gez73LbDdn0PuSsqduLI0GfWhUXcfXI2O813tw89qjUkxCqsUM26cn/sqbPhwP3ljLf0DVYJQkOdDbM8X/rF3qX/e9xW4gqrvN7NFKMh6+OdG3aFRD2h0rG52TlMKqxR0ww1FZxL+05/ghz8MrByRox05DLmrfHjt/dJ3sd/7pX8U7Kvevuu38feGNQo9wq+zukHd7NjUL0lHYZWC9u6Fvn0hJ8cv163rbxY+8cRAyxIpn3N+IN+9y3yLbO+y0Otl/lqZK6ze/jOb+dAKPxqFX3f1IadrZClLYZWipk+HM86ITCXSsyd8/DE0ahRsXSJVduSQvwa27yvYu9w/7ws9V2ak+tLUyoSsztCwK2R18QGW1cV3xc/qoutkSU5hlcJ+8hP4/e8jy6NH+wFwNfeV1Dj5uZC7AvaFHtGv8zbE5jvqZEfCq2Hn0OvO0LCTD7k6jWPzPVIlCqsUdviwH8li5szIOl2/krRTcMBfH8tdCftW+ufw6/051evoEa1Ok1DLrLMPsAYd/XPDTtCwo+8Uon8pxo3CKsVt3OhHt9iyxS9nZPjegWeeGWhZIsmhsMCPYJ+72gfa/tBz+HFoe+y+K6NeJMAadPAB9r/n0GvNO1ZlCqsa4KOPfAvrSOiezZYt/ViC7doFW5dI0svf51tfuTmhIAs/r/aTZObvju33ZTbzodWgAzRoX8Lrdj705CgKqxriL3/xI7KHDR0KH3zgewqKSBUd3uNDa39O6LEm6nktHNoW++/MbO6ncmnQPvKo384HWfi5TnbanXJUWNUQzvnxAl95JbLu9tvh738PriaRGq/ggD/NGA6v/WvgwNrQutBz4eHYf29Gg0h41W8bet028mjQznfVr0GtNIVVDZKbCyedBF98EVk3Zgxcc01wNYmkNVcIB7f5AAuH14H1oefQ67yNVR93sTx1m4YCrA3UawMN2vrn+sUeKTAqiMKqhlm2DAYPhn2hQQLq14c5c3TDsEjSKiyAg5vhwIZQkK2HvPXFljfEp4UWVrsR1G8dCbX6rf0AxMWfM1sENqu0wqoGev11uPzyyHKHDjB3LrRpE1xNIlINzvmei3kbfIgVed7oW2d5G+NzDa0Ig3otoF4rH171WoWCrFWxR8tQsNWO3TfHIqzM7Fbg3865PWb2D+Ak4C7n3Ecxq7QK0jWsAO69F/7wh8jygAHw4YeQpUleRWquI4cjrbS8jZC3yT8f3BR5nbcptl32S2W+92O9lj7AWo+E439a9b3FKKw+dc6daGanAr8NPX7lnBtS5cpiIJ3D6sgR37p6663Iuosv9q2ujGBa8SKSLI4choNbfHAd3BwKsk2RUDu4BfI2+/didfqx09Vw6stV/nhZYVWZ9lv4FvGzgBedc++Z2e+qXJVUW0aGn0n4jDNgwQK/7u234c474W9/S7teryISLaMuNOzgH2Vxzt9rlrc5EmIHt/gQ+1+ghZe3lt1RpF6r2P4MUSoTVoVm9g3g68BFoXW6wydgDRvC+PG+h+DatX7do49Ct24+tEREymTmexTWbepnei6LK4TDu6ICbWvR1y1OjVuZlQmr24F7gaecczlm1gOYFp+ypDJat4YJE+CUU/zUIuBvHu7cGS69NNDSRKQmsVr+GlVmM8jundivrkpvQDMzIMs5V80Z1qovna9ZFTdlCpx3HhSETtjWr+87XAweHGxdIiIVUdY1qwrPUmZmz5hZEzOrC3wCbDGz78aqSKm+ESPgyScjy3l5cNFFsHp1cDWJiMRCZabUHOic2w2cCywCWgO3xqUqqbIbb4Sf/zyyvHUrjBzpR24XEUlVlQmrcN+yYcB459xeoJrzU0s8PPggXH11ZHnVKh9Y2+J9L6GISJxUJqw2m9njwBXAZDOrA+huniRkBs89B+efH1m3dCmcey7sjvFsCCIiiVCZsLoG+BL4Ruh0YDvg4bhUJdVWty6MG+fvwQpbtAguvNAPhisikkoqHFbOue3AE4AzsyHAFufc8/EqTKqvfn1/k/CQqDFGZs2CSy6BgweDq0tEpLIq0xvwFGAl8DjwJLDCzIbGqzCJjUaNYOLEoiOyT50KV14J+fnB1SUiUhmVOQ34MHCFc66/c64f/trVX+JTlsTSMcfA++9Djx6RdW+/Dddd58cXFBFJdpUJq3rOuZnhBefcLKDmTFFZw7VqBZMnQ6dOkXWvvALXXqsWlogkv8qE1QEzGxleMLMzgQMxr0jipkMHP8pF9JxX//43fP3rcOhQcHWJiJSnMmH1feAZM1tuZsuA54HvxaUqiZtu3XxgtW4dWReeyDEvL7i6RETKUqmxAUP3Vh2Hv4A9EP8AABa9SURBVEH4S+dc4CeQNDZg1Xz1lR+ead26yLqzzvJzYzVsGFxdIpK+qjU2oJk1CD+AOsAqfK/AOqF1koK6d4ePPoIuXSLrpk71Nw6HR24XEUkWFTkNmAvsCz2HX++Lei0pqnNnmD69aC/BmTP90Ew7dwZWlojIUcoNK+dcLedcRug5/Dq8rOGWUly7dr6F1adPZN28ef6U4ObNwdUlIhKtMh0sqsTMupvZrFDHjI/NrNQZu8yshZltMbP/xrsuiWjVCqZNg/79I+sWL/aTOS5fHlxdIiJhcQ8r/BBNTzrnegAPAc+Use1jwIQE1CTFNG/ur1mdfHJk3erVPrBmzw6uLhERiHNYmVlLYAAwJrRqHNDFzDqXsO01wBbgw3jWJKVr0gQmTSo6WvuOHf6U4JtvBleXiEi8W1YdgI3OuQIA5/vJrwU6Rm9kZm2Bu4B741yPlCMrywfTt74VWXfwoL8P6/HHg6tLRNJbIk4DFr+Ry0rY5ingx865cievMLO7zGx9+JGr+S5irk4dePppuO++yLrCQvjOd/wsxJW4NU9EJCYqdVNwpXfuTwN+BTRzzhWYmQGbgJOdczlR2+0Ewnf3ZAH1gRnOuXPL+w7dFBxfTz0Ft93mwyrs+uvhySchMzO4ukSk5qnWTcHV4ZzbCiwCrg2tGgXkRAdVaLtjnHOdnXOdgbuBiRUJKom/W27xpwXr14+se/FFP/rF1q3B1SUi6SURpwFvBW41s+X4a1I3AZjZBDMblIDvl2q66CLftb1588i6mTNh0CD45JPg6hKR9BHX04CJoNOAibNyJVx8MSxdGlnXoIFvaY0aFVxdIlIzBHYaUGqWbt1gzhy48MLIugMHYPRouP/+ote1RERiSWElldK4sb+Gdc89Rdc/8ABceSXs3x9MXSJSsymspNIyMuD3v4cxY4r2CBw3zo94sWJFcLWJSM2ksJIqu+YaPwhu9MzDn34KAwfCa68FV5eI1DwKK6mWIUNg/nwYPDiybu9e3+Hihz+E/MCn5xSRmkBhJdXWtq1vYX3nO0XXP/wwDB8OGzYEU5eI1BwKK4mJevXgscf8dawGUfNHz5zppx6ZPDm42kQk9SmsJKauucZP3tizZ2Tdtm1wzjm+x2BBQXC1iUjqUlhJzPXu7QPrqqsi65zz92KdeSbk5ARUmIikLIWVxEVWFrz8sj81WLduZP3MmdC3L4wdG1xtIpJ6FFYSN2a+08WsWdCjR2T93r3+dOF11/nXIiLlUVhJ3A0cCAsXws03F10/Zgz06+fDTESkLAorSYiGDf3cWOPGQdOmkfWrV8OwYX6ix8OHg6tPRJKbwkoS6vLL/SgXZ50VWXfkCPzqV/4GY005IiIlUVhJwrVvD5MmwUMPQZ06kfWLF/uRMB54QCNfiEhRCisJRK1a8KMfwdy5cMIJkfUFBb6L+5AhPrxEREBhJQHr39+PLfiLX/jR3MM++cS3sn71K7WyRERhJUmgbl148EHfyurTJ7I+P993vBg0yL8nIulLYSVJY+BA38r62c+KtrI+/RSGDoXbb9d9WSLpSmElSSUzE379a5gzp2gryzn4xz+gVy94/fXg6hORYCisJCkNGgQLFsBvflN0NuKNG33390svhXXrgqtPRBJLYSVJq25d+OlP4bPPYMSIou+9+aYfMPfhh9UBQyQdKKwk6R17rL8v68UXoXnzyPrcXD8bcd++mi9LpKZTWElKMPMD3y5dCjfcUPS9pUvh7LNh9GhYsyaQ8kQkzhRWklKaN4fnnoMPPih6MzH4cQd79fIdNA4eDKQ8EYkThZWkpDPO8CO5/+1vkJ0dWZ+X528wPv54eO0134tQRFKfwkpSVu3a8L3vwfLlcNNNRd9btQpGjfKhNm9eMPWJSOworCTltWwJTz/tR7kYPLjoe9On+3EGr70W1q4Npj4RqT6FldQYQ4b4m4mffRbatCn63ssvw3HH+a7wGgVDJPUorKRGqVULbrzRnxr85S+hQYPIewcPwu9+B927w9//DocOBVeniFSOwkpqpKwsP9XI8uW+q7tZ5L2tW+H734eePeGll/zkjyKS3BRWUqO1a+e7ui9YAMOHF30vJweuv97fVPzWW+o5KJLMFFaSFvr3hylTYPx4OPHEou99/jlccgmceqq/f0tEko/CStKGGVx4ISxa5DtcdO1a9P3Zs33ra8QI34tQRJKHwkrSTq1acPXVfpimxx6D1q2Lvj91KgwbBiNHwsyZwdQoIkUprCRt1a0L3/kOrFjhewk2aVL0/SlT4LTT4JxzfKtLRIKjsJK017Ah3Huv73DxwANFh28CP+L7KafAuefCRx8FUqJI2lNYiYRkZ8N99/nQ+uUvoXHjou+//74fvun002HiRPUeFEkkhZVIMU2a+Hu0cnJ8eBUPrRkz4IILYOBA+O9/dZ+WSCIorERK0bSpPy24erVvaTVtWvT9RYvgiiv8CO/PPacRMUTiSWElUo5jjvEtrTVr4I9/PLr34LJl8K1vQZcu8Ic/wO7dgZQpUqMprEQqqFEjuPtu39J67DHo3Lno+5s2+Y4aHTrAXXdp1mKRWIp7WJlZdzObZWbLzexjM+tdwjaXmdmnZvaJmX1uZr8xix7NTSR51Kvnu7wvXw4vvOBPA0bLzYW//AW6dYNrrvGTRIpI9SSiZfUE8KRzrgfwEPBMCdtMBvo55/oB/YGzgYsTUJtIldWp48cWXLIEJkw4euzBI0dg7FjfEeP0031njIKCYGoVSXVxDSszawkMAMaEVo0DuphZ5+jtnHP7nHOFocV6QCZQiEgKMIPzz/cjX8yfD9/4BmRkFN1mxgzfGaNbN3joIdi5M5haRVJVvFtWHYCNzrkCAOecA9YCHYtvaGanmNmnwFZgCvBOSTs0s7vMbH34kZubG7/qRSpp4ED417/8qBh33OGnKom2di3cc4+/rnXbbfDFF8HUKZJqEnEasPitkyVei3LOzXLOnYgPuMHA6aVs97Bzrn34kVX8r4FIEujcGR55BNav99evunQp+v6BA/DEE/561/Dh8J//QH5+IKWKpIR4h9U6oL2Z1QYIdZrogG9dlcg5tw3fqroizrWJxF12Ntx5J3z1FbzxxtHXtcBPS3LlldCpk7+fa8OGhJcpkvTiGlbOua3AIuDa0KpRQI5zLid6OzM7zsxqhV43Ai4CPo1nbSKJlJHh58yaOhUWL/b3ZdWrV3SbTZvgwQd9aI0eDZMnQ6Gu3IoAYC7OA5yZ2XHA80AzYC/wTefc52Y2AbjPOTffzH4OXA3kAxnAf4EHXAWKa9++vVu/fn3c6heJl507/cgX//wnrFxZ8jZdu8LNN8ONNx59M7JITWNmG5xz7Ut8L95hFW8KK0l1hYV+ZPfHHvMzGZfUmqpdGy6+GL79bTj77KN7G4rUBAorkRSxZo3vePHss7BlS8nbdOoEN9zgH8VH0RBJZQorkRSTnw9vvw1PPQXvvVf6dCRnneWvf112GTRokNgaRWJNYSWSwnJy4JlnfGtr48aSt2nc2N+M/K1vwZAh/kZlkVSjsBKpAQoK/LBOzz7rr22VNo9Wjx5+GKhrr/WnDEVShcJKpIbZsgXGjPHBVdYoGMOGwXXX+aGesrMTV59IVSisRGoo52DePN8F/l//gj17St6uXj342tf8KPDnnQd16ya2TpGKUFiJpIG8PN8p48UX4d13Sz9N2LQpjBoFV1/tW17qBi/JQmElkma2bvUtrZdeggULSt+ubVvfMeOqq/wgvOqYIUFSWImksS++8KE1dqwf9b00Xbv6MQqvvBL69VNwSeIprESEwkKYPduH1quvwvbtpW/bvXskuE44QcEliaGwEpEi8vNhyhR/qvC116CsaeF69PDXuEaNggEDFFwSPworESlVXp7vkPHKK76DxoEDpW/buXMkuE46CWolYkY8SRsKKxGpkAMH/I3Hr77qbzzOyyt927Zt/TBPl13mexXWqZO4OqVmUliJSKXl5sI778C4cT7A9u8vfdsmTeDCC+HSS/19XJrAW6pCYSUi1ZKX5wfUHTfOnyos7eZjgMxMGDnSTzZ50UXQpk3i6pTUprASkZg5fNh3znjtNXjrLX9PV1kGDfJzcV18sbrES9kUViISF0eOwJw58Oab8PrrsGJF2du3b+9bWxddBMOHa1oTKUphJSJx5xwsXQpvvOEf8+aVvX29ej6wLrwQLrgAunRJTJ2SvBRWIpJwmzb5Dhpvvw2TJpXdsxCgZ08fWhdcAKed5q99SXpRWIlIoPLyYOpUH1zjx8OGDWVv36CBnwX53HN978Jjj01MnRIshZWIJA3nYMkS3+qaMAFmzfJDQZWlWzcfWuecA2ee6WdGlppHYSUiSWvnTnj/fR9cEyeWPWYhQO3acPLJcPbZPrwGDfLrJPUprEQkJRQWwqJFfvind9/1A++WNi9XWHa2P2U4ciSMGOHHMlT3+NSksBKRlLR7t7/WNXGib32VNcVJWLt2keAaMcIPCyWpQWElIinPOfjqKx9akyb5ECtrtPiwnj19F/nhw/31rhYt4l6qVJHCSkRqnPx8f0PypEkweTJ8/HH5pwwB+vSJhNcZZ8Axx8S/VqkYhZWI1Hh798JHH/ngmjIFPvus/M+YwYkn+tAaNsw/1PIKjsJKRNLOli3+VOG0af5R3lBQYb17+/A64ww4/XRd80okhZWIpL116+CDDyLhlZNTsc917epDK/zo3l29DeNFYSUiUkxODnz4oQ+wDz+E1asr9rlWrfxwUKee6h/9+2viyVhRWImIlGPdOh9a4cdXX1Xsc/Xrw0knRcJr6FA/GaVUnsJKRKSSNm+GGTNg+nTfcWPxYt99vjxm0KsXnHKKD66hQ+G446BWrfjXnOoUViIi1bRnjx/HcPp0mDnTd5U/eLBin23a1A8RNXSob4UNGaLWV0kUViIiMXb4sB8aauZM/5gxo/xZk6P16uUDLPw4/njIyIhfvalAYSUiEmfOwcqVfjzD2bN9K2zJkvJHlA9r2BAGDvStrvCjY8f06nmosBIRCcC+ff50YTi85s71o8xXVKtWPrQGDYLBg/2jefP41Rs0hZWISBJwzt+cPGeOD645c3zHjYKCiu+jU6dIcA0a5Ftj2dnxqzmRFFYiIkkqLw8WLvQtsPBj1arK7ePYY31ohR8DBqRmBw6FlYhICtm+HebN88E1dy7Mnw/btlVuH926RYJrwAB/83Kyn0JUWImIpDDn/Fxe8+f7EJs3z7/eu7dy++nQIRJe/fr5AGvfPnk6cSisRERqmMJCf/1rwYLIY+HCygdYs2Y+uMLh1a+fv4m5du341F2WQMPKzLoDLwDNgd3ADc65L4pt83XgXqAO4IAnnXN/r8j+FVYiIl7xAFu0yAfY7t2V209mpr/vq29f/+jXz0+l0rRpfOoOCzqspgIvOueeN7PRwA+dc0OLbXMqsNI5t9nMsoEFwDedczPL27/CSkSkdM7BmjU+tBYu9CH2ySd+OKnK6tjRh1b0o3v32LXCAgsrM2sJLAeaO+cKzMyATcDJzrmcMj43Hvi3c25Med+hsBIRqbzNm31ohR+LFvnBeysbCZmZfg6wE0/0U6jcdFPVawoyrAYCLznneket+xi42zn3USmf6Q1MB05wzm0s7zsUViIisZGb60fdWLw48vj0U9i/v2KfP+88mDix6t9fVlgl4hJa8TQstd+JmbUH3gRuKy2ozOwu4K7wcnZNuRtORCRgWVmRkeLDCgv9MFLh4Ao/Spr/68QT41dbIk4DfgU0K+80oJm1BaYCv3POvVDR71DLSkQk8fbuhc8+8y2xcIDdeSeMGlX1fQbWsnLObTWzRcC1wPPAKCCnhKBqA0wB/lCZoBIRkWA0buzn7DrllMR8XyKmA7sVuNXMluO7p98EYGYTzGxQaJsHgY7AHWb2SehxYwJqExGRFKCbgkVEJCmUdRpQEy2LiEjSU1iJiEjSU1iJiEjSU1iJiEjSU1iJiEjSU1iJiEjSU1iJiEjSS/n7rMzsEFDJCZ+PkgXkxqCcmkTH5Gg6JkfTMTmajsnRKnpMWjjnMkt6I+XDKhbMbH1pN6KlKx2To+mYHE3H5Gg6JkeLxTHRaUAREUl6CisREUl6Civv4aALSEI6JkfTMTmajsnRdEyOVu1jomtWIiKS9NSyEhGRpKewEhGRpJfWYWVm3c1slpktN7OPzax30DUlmpn9zcxyzMyZWZ+o9Wl7bMysnpm9EfrZPzGzd82sc+i9lqHlr8zsMzM7LdhqE8fM3jezT0PHZLqZ9QutT9vfFQAz+2X0/z86HpZjZl9GTaT79dD66h0X51zaPoCpwA2h16OB2UHXFMAxGAa0B3KAPjo2DqAecAGRa7q3A++HXj8L3B96PRhYA9QOuuYEHZcmUa8vBRbqd4UBwMTQ70GfdD8eoZ+5yN+SqPXVOi6B/2ABHtCWwO7wHxrAgM1A56BrC+h45ET9z6ZjU/TYDAJWhF7n4u+yD7/3MXBm0DUGcEy+CcxP598VIBOYDXQJ//+Tzscj6rgcFVaxOC7pfBqwA7DROVcA4PwRXAt0DLSq5KBjU9T3gbfNrBlQyzkXPbxXDml0XMzsRTNbB/waH1jp/LvyIDDGObc6al06H49oL5vZEjN72sxaEIPjks5hBVC8374FUkVy0rEBzOynQHfgZ6FVaX1cnHPXO+c6AD8H/hheXWyzGn9MzGwo/jTwYyW8nXbHo5hhzrm++FOkO4AXQuurdVzSOazWAe3NrDaAmRk+/dcGWlVy0LEBzOxu4HLgfOfcAefcjtD6FlGbdSLNjguAc+4FYDiwnvT8XTkD6AmsNrMc/HXf9/CnAtPxePyPc25t6DkfeAQ4nRj8TUnbsHLObQUWAdeGVo0CcpxzOYEVlSR0bMDM7gKuAs52zu2Oeus/wP+FthkMtAZmJL7CxDKzxmbWNmr5Mvy/mtPyd8U593vnXFvnXGfnXGd8aJ8bCvG0Ox5hZtbQzJpErboKWBSLvylpPYKFmR0HPA80A/YC33TOfR5oUQlmZv8ALsH/0d0O5Drnjk3nY2Nm7fH/ElwF7AutPuScO8nMWgEv4S+qHwa+65z7MJhKE8fMOgDjgPpAIX5anrudc5+k8+9KWKh1dZFz7rN0Ph5m1hX/e5KBP823CrjDOZdT3eOS1mElIiKpIW1PA4qISOpQWImISNJTWImISNJTWImISNJTWImISNJTWImISNJTWIkkkJndb2Z1Q68fDE+fICJl031WIglkZg5o5JzLDboWkVSilpVIgpjZ46GXs0KT0k0ws9tD791vZv8ys/FmtsLMXjWz/mY21cxWmdnDUftpHXr/49BkiA+G1tcys0dDE98tNrMFZlYvgB9VJObUshJJoOiWlZk9D8x3zj1qZvcD1+DnzsoFFuLn+/kaUBtYDZzmnFtuZu8Bv3HOfRQaGHQ88AR+upKxwPHOuUIzywb2OecKE/pDisRB7aALEJH/ec85twfAzD4FFjvnDgGHzGwZ0NXMNgBnAa38wNUAZOFHAJ8K1AGeNbNpwDsKKqkpFFYiyeNg1OsjJSzXxp+6d8Dg0BQMRZjZ8fjpK4YDvzOzYc65FfErWSQxdM1KJLH2AdlV/bBzbh8wHbg3vM7M2ppZ+9A8Ww2dc+8DP8WfFuxdvXJFkoNaViKJ9WdgqpnlARuruI9rgIfNbEloORe4DT8tw1NmVgf/D9FZwMRq1iuSFNTBQkREkp5OA4qISNJTWImISNJTWImISNJTWImISNJTWImISNJTWImISNJTWImISNJTWImISNL7fzhSqQZNcBLeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 480x344 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the training and validation history per epoch\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 4.3), dpi=80)\n",
    "plt.title(\"Training Monitoring\")\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(trl,color='blue',linewidth=3.0)\n",
    "plt.plot(devl,color='orange',linewidth=3.0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:56.489814Z",
     "start_time": "2020-02-15T14:37:56.487014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.825\n",
      "Precision: 0.9276315789473685\n",
      "Recall: 0.705\n",
      "F1-Score: 0.8011363636363635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Compute accuracy, precision, recall and F1-scores\n",
    "\n",
    "preds_te_tfidf=predict_class(test_vector_tfidf,w_tfidf)\n",
    "\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_tfidf))\n",
    "print('Precision:', precision_score(Y_te,preds_te_tfidf))\n",
    "print('Recall:', recall_score(Y_te,preds_te_tfidf))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Print top-10 most positive and negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:38:17.845485Z",
     "start_time": "2020-02-15T14:38:17.842557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_list: ['great', 'truman', 'life', 'perfectly', 'hilarious', 'terrific', 'perfect', 'memorable', 'excellent', 'world']\n",
      "negative_list: ['bad', 'worst', 'boring', 'supposed', 'why', 'unfortunately', 'harry', 'stupid', 'ridiculous', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "positive_list,negative_list=top_words(w_tfidf.tolist())\n",
    "print('positive_list:',positive_list)\n",
    "print('negative_list:',negative_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  learning rate is 0.0001,because at this time, the curve drops most smoothly,But when the epochs is more than 20, the change of loss becomes very small.Regularisation strength is 2*weights*lr*alpha.This regularisation can mprove the precision of the model.\n",
    "2 The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
    "3 In my experiment, regularisation strength has little effect on the experimental results, but it does improve the precision of the model.This is because regularization avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |  0.894 | 0.760  |  0.821 |\n",
    "| BOW-tfidf  | 0.927  | 0.705  | 0.801  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression \n",
    "\n",
    "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class 1: World\n",
    "- Class 2: Sports\n",
    "- Class 3: Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to follow the same process as in Task 1 for data processing and feature extraction by reusing the functions you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.212229Z",
     "start_time": "2020-02-15T14:18:03.185261Z"
    }
   },
   "outputs": [],
   "source": [
    "data_tr = pd.read_csv('data_topic/train.csv',header=-1)\n",
    "data_dev = pd.read_csv('data_topic/dev.csv',header=-1)\n",
    "data_test = pd.read_csv('data_topic/test.csv',header=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1  1  Reuters - South Korean police used water canno...\n",
       "2  1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3  1  AFP - Sporadic gunfire and shelling took place...\n",
       "4  1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.806523Z",
     "start_time": "2020-02-15T14:18:03.798279Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawX2(data):\n",
    "    rawlist=[]\n",
    "    for i in range(0,len(data)):\n",
    "        rawlist.append(data[i][1])\n",
    "       \n",
    "    return rawlist#  a list of strings each corresponding to the raw text of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_list=data_tr.values.tolist()\n",
    "development2_list=data_dev.values.tolist()\n",
    "test2_list=data_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_list_ngram=[]\n",
    "for i in range(0,len(train2_list)): \n",
    "    a=extract_ngrams(train2_list[i][1], ngram_range=(1,3), stop_words=stop_words)\n",
    "    train2_list_ngram.append(a)\n",
    "development2_ngram=[]\n",
    "for i in range(0,len(development2_list)): \n",
    "    a=extract_ngrams(development2_list[i][1], ngram_range=(1,3), stop_words=stop_words)\n",
    "    development2_ngram.append(a)\n",
    "test2_ngram=[]\n",
    "for i in range(0,len(test2_list)): \n",
    "    a=extract_ngrams(test2_list[i][1], ngram_range=(1,3), stop_words=stop_words)\n",
    "    test2_ngram.append(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_raw2=get_rawX2(train2_list)\n",
    "X_dev_raw2=get_rawX2(development2_list)\n",
    "X_test_raw2=get_rawX2(test2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_train2, df_train2, ngram_counts_train2 = get_vocab(X_tr_raw2, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_vector=vectorise(train2_list_ngram, vocab_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2_vector=vectorise(development2_ngram, vocab_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_vector=vectorise(test2_ngram, vocab_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to change `SGD` to support multiclass datasets. First you need to develop a `softmax` function. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.440998Z",
     "start_time": "2020-02-15T14:18:07.437915Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "\n",
    "    smax=[]\n",
    "    if z.ndim==1:\n",
    "  \n",
    "        smax = (np.exp(z))/(np.sum(np.exp(z)))\n",
    "    else:\n",
    "        smax = (np.exp(z))/(np.sum(np.exp(z),axis=1).reshape(-1,1))\n",
    "\n",
    "    \n",
    "    return smax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then modify `predict_proba` and `predict_class` functions for the multiclass case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.445451Z",
     "start_time": "2020-02-15T14:18:07.442851Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "  \n",
    "    z=np.dot(X,weights.T) # x:1.5000   w:3.5000\n",
    "   \n",
    "    preds_proba=softmax(z)  #Output 3 possibilities per file\n",
    "    \n",
    "    \n",
    "    return preds_proba  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.449814Z",
     "start_time": "2020-02-15T14:18:07.447145Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "\n",
    "    predict_class_=[]\n",
    "    for i in range(0,len(X)):#Three possibilities per document\n",
    "        preds_proba=predict_proba(X[i], weights) #Three possibilities\n",
    "        \n",
    "        preds_class=np.argmax(preds_proba)  #Add in the most likely one\n",
    "        \n",
    "        predict_class_.append(preds_class+1)\n",
    "    return predict_class_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example and expected functionality of the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.059902Z",
     "start_time": "2020-02-15T14:18:08.056774Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.1,0.2],[0.2,0.1],[0.1,-0.2]])\n",
    "w = np.array([[2,-5],[-5,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33181223, 0.66818777],\n",
       "       [0.66818777, 0.33181223],\n",
       "       [0.89090318, 0.10909682]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba(X,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.714215Z",
     "start_time": "2020-02-15T14:18:08.710098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:30:48.047338Z",
     "start_time": "2020-02-15T14:30:48.044395Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, weights, num_classes=5, alpha=0.00001):\n",
    "    #  X  ï¼š 2400*5000   wï¼š3*5000\n",
    "\n",
    "\n",
    "    preds_proba_list=predict_proba(X, weights) #2400 * 3\n",
    "   \n",
    "    predict=predict_class(X, weights)  # 2400*1\n",
    "    \n",
    "    weights=np.asarray(weights)\n",
    "    loss_=[]\n",
    "\n",
    "    for i in range(0,len(preds_proba_list)):\n",
    "        l_=0  \n",
    "        if predict[i]==Y[i]:\n",
    "            if predict[i]==1:                \n",
    "                l_=-np.log(preds_proba_list[i][0])*1 \n",
    "            if predict[i]==2:\n",
    "                l_=-np.log(preds_proba_list[i][1])*2     \n",
    "            if predict[i]==3:\n",
    "                l_=-np.log(preds_proba_list[i][2])*3 \n",
    "        \n",
    "\n",
    "        loss_.append(l_)\n",
    "\n",
    "    l=sum(loss_)/X.shape[0]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support the categorical cross entropy loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you MLR following the same steps as in Task 1 for both Count and tfidf features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Y list from train and dev data\n",
    "Y2_tr=[]\n",
    "for i in range(0,len(train2_list)):\n",
    "    Y2_tr.append(train2_list[i][0])    \n",
    "Y2_tr=np.asarray(Y2_tr)\n",
    "Y2_tr=Y2_tr.reshape(Y2_tr.shape[0],1)\n",
    "Y2_dev=[]\n",
    "for i in range(0,len(development2_list)):\n",
    "    Y2_dev.append(development2_list[i][0])   \n",
    "Y2_dev=np.asarray(Y2_dev)\n",
    "Y2_dev=Y2_dev.reshape(Y2_dev.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], num_classes=5, lr=0.01, alpha=0.00001, epochs=5, tolerance=0.001, print_progress=True):\n",
    "    \n",
    "    \n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    weights=np.zeros(((3,len(X_tr[0]))))\n",
    "    weights=np.asarray(weights)\n",
    "    #get Y\n",
    "    Y=np.zeros((len(Y_tr),3))\n",
    "    for i in range(len(Y_tr)):\n",
    "        if Y_tr[i]==1:\n",
    "            Y[i][0]=1\n",
    "        if Y_tr[i]==2:\n",
    "            Y[i][1]=1\n",
    "        if Y_tr[i]==3:\n",
    "            Y[i][2]=1\n",
    "    \n",
    "    for j in range(0,epochs):\n",
    "       \n",
    "        for i in range(len(X_tr)):\n",
    "\n",
    "            weights=weights-(lr*np.dot((predict_proba(X_tr[i],weights)-Y[i]).reshape(-1,1),X_tr[i].reshape(-1,1).T))-2*weights*lr*alpha\n",
    "        #get train_loss\n",
    "        train_loss=categorical_loss(X_tr,Y_tr, weights, alpha=0.00001)\n",
    "        #get  validation_loss\n",
    "        validation_loss=categorical_loss(X_dev, Y_dev, weights, alpha=0.00001)\n",
    "       \n",
    "        print('epochs:',j,'trainingloss:',train_loss,'validation_loss:',validation_loss)\n",
    "        training_loss_history.append(train_loss)\n",
    "        validation_loss_history.append(validation_loss)\n",
    "     \n",
    "        \n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:55.324956Z",
     "start_time": "2020-02-15T14:18:11.720952Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 trainingloss: 0.7482141895694419 validation_loss: 1.0625890937404094\n",
      "epochs: 1 trainingloss: 0.5769173228381647 validation_loss: 0.8887030519527036\n",
      "epochs: 2 trainingloss: 0.49663495347141046 validation_loss: 0.7890718306439675\n",
      "epochs: 3 trainingloss: 0.4518054930479967 validation_loss: 0.7455031212224816\n",
      "epochs: 4 trainingloss: 0.41905673378268865 validation_loss: 0.7212227585556968\n",
      "epochs: 5 trainingloss: 0.3966244012011407 validation_loss: 0.6897170218329347\n",
      "epochs: 6 trainingloss: 0.37900633419475405 validation_loss: 0.6516696953931818\n",
      "epochs: 7 trainingloss: 0.36444217851916705 validation_loss: 0.6399383060127695\n",
      "epochs: 8 trainingloss: 0.3530826302108728 validation_loss: 0.6255219306127877\n",
      "epochs: 9 trainingloss: 0.34417194843244364 validation_loss: 0.6140112637186897\n",
      "epochs: 10 trainingloss: 0.33531458904426864 validation_loss: 0.6047110781121825\n",
      "epochs: 11 trainingloss: 0.32769092822685353 validation_loss: 0.5971265899160423\n",
      "epochs: 12 trainingloss: 0.3220306873529473 validation_loss: 0.5908953549002577\n",
      "epochs: 13 trainingloss: 0.3190759013505306 validation_loss: 0.585745415187689\n",
      "epochs: 14 trainingloss: 0.31568457748647444 validation_loss: 0.5617988760109436\n",
      "epochs: 15 trainingloss: 0.31144139638914525 validation_loss: 0.5581048650171644\n",
      "epochs: 16 trainingloss: 0.30725983144814883 validation_loss: 0.5550001608934777\n",
      "epochs: 17 trainingloss: 0.30335065111416243 validation_loss: 0.5523830033311976\n",
      "epochs: 18 trainingloss: 0.30109236915447196 validation_loss: 0.5501713559616264\n",
      "epochs: 19 trainingloss: 0.300402377057774 validation_loss: 0.5482985296045381\n",
      "epochs: 20 trainingloss: 0.2986157152916765 validation_loss: 0.546709921527647\n",
      "epochs: 21 trainingloss: 0.2971992190672841 validation_loss: 0.5453605468024555\n",
      "epochs: 22 trainingloss: 0.2959631570684342 validation_loss: 0.5330484554617052\n",
      "epochs: 23 trainingloss: 0.2943627492541656 validation_loss: 0.5320274456067862\n",
      "epochs: 24 trainingloss: 0.2930443195577302 validation_loss: 0.5311549270932272\n",
      "epochs: 25 trainingloss: 0.2915120689054649 validation_loss: 0.53040894825824\n",
      "epochs: 26 trainingloss: 0.29013946165888743 validation_loss: 0.5297710006164285\n",
      "epochs: 27 trainingloss: 0.29025177624630455 validation_loss: 0.5292254317554627\n",
      "epochs: 28 trainingloss: 0.289137178121806 validation_loss: 0.5287589688490665\n",
      "epochs: 29 trainingloss: 0.2881334785772474 validation_loss: 0.5283603295596297\n",
      "epochs: 30 trainingloss: 0.2872282558467101 validation_loss: 0.5280199025330305\n",
      "epochs: 31 trainingloss: 0.2864106571152497 validation_loss: 0.5277294837176956\n",
      "epochs: 32 trainingloss: 0.2856711753573688 validation_loss: 0.5274820577626428\n",
      "epochs: 33 trainingloss: 0.2857599750979844 validation_loss: 0.527271616039579\n",
      "epochs: 34 trainingloss: 0.2851499362399969 validation_loss: 0.5217947540188429\n",
      "epochs: 35 trainingloss: 0.2856268766820402 validation_loss: 0.5216315445081646\n",
      "epochs: 36 trainingloss: 0.28541309282868815 validation_loss: 0.5214928658172255\n",
      "epochs: 37 trainingloss: 0.28495072361590407 validation_loss: 0.5213753640153828\n",
      "epochs: 38 trainingloss: 0.28452949406412753 validation_loss: 0.5212761446659016\n",
      "epochs: 39 trainingloss: 0.2857210455922684 validation_loss: 0.5211927074940438\n",
      "epochs: 40 trainingloss: 0.2853668999560689 validation_loss: 0.5211228908496386\n",
      "epochs: 41 trainingloss: 0.2850432585173484 validation_loss: 0.5210648243999667\n",
      "epochs: 42 trainingloss: 0.28474723082154263 validation_loss: 0.5210168887574586\n",
      "epochs: 43 trainingloss: 0.2847846869929474 validation_loss: 0.520977680965272\n",
      "epochs: 44 trainingloss: 0.28453615242155583 validation_loss: 0.5209459849425209\n",
      "epochs: 45 trainingloss: 0.2846259234695564 validation_loss: 0.520920746137627\n",
      "epochs: 46 trainingloss: 0.2844164086776862 validation_loss: 0.5209010497592452\n",
      "epochs: 47 trainingloss: 0.2842240007367049 validation_loss: 0.5208861020543093\n",
      "epochs: 48 trainingloss: 0.28404717810166985 validation_loss: 0.5208752141859029\n",
      "epochs: 49 trainingloss: 0.28388456773821424 validation_loss: 0.5208677883328999\n",
      "epochs: 50 trainingloss: 0.2837349291699314 validation_loss: 0.5208633056912284\n",
      "epochs: 51 trainingloss: 0.2835971404078094 validation_loss: 0.5208613161050721\n",
      "epochs: 52 trainingloss: 0.28347018551915243 validation_loss: 0.5208614290970915\n",
      "epochs: 53 trainingloss: 0.2833531436273993 validation_loss: 0.5208633061010056\n",
      "epochs: 54 trainingloss: 0.28324517916301134 validation_loss: 0.5208666537288521\n",
      "epochs: 55 trainingloss: 0.283523571784529 validation_loss: 0.5208712179296635\n",
      "epochs: 56 trainingloss: 0.2834312204993563 validation_loss: 0.5208767789170478\n",
      "epochs: 57 trainingloss: 0.28334589320025266 validation_loss: 0.5208831467607333\n",
      "epochs: 58 trainingloss: 0.28359294981238115 validation_loss: 0.5208901575520697\n",
      "epochs: 59 trainingloss: 0.28351992294759126 validation_loss: 0.5208976700662065\n",
      "epochs: 60 trainingloss: 0.2834523514133761 validation_loss: 0.5209055628545041\n",
      "epochs: 61 trainingloss: 0.2833897993513734 validation_loss: 0.5209137317099898\n",
      "epochs: 62 trainingloss: 0.2833318684548692 validation_loss: 0.520922087456573\n",
      "epochs: 63 trainingloss: 0.283278194457469 validation_loss: 0.5209305540195123\n",
      "epochs: 64 trainingloss: 0.28322844397931596 validation_loss: 0.5209390667404232\n",
      "epochs: 65 trainingloss: 0.2831823116912218 validation_loss: 0.5209475709050874\n",
      "epochs: 66 trainingloss: 0.28313951776186325 validation_loss: 0.5209560204566036\n",
      "epochs: 67 trainingloss: 0.2830998055573057 validation_loss: 0.5209643768700917\n",
      "epochs: 68 trainingloss: 0.2830629395657537 validation_loss: 0.5209726081683274\n",
      "epochs: 69 trainingloss: 0.28302870352357073 validation_loss: 0.5209806880604023\n",
      "epochs: 70 trainingloss: 0.282996898721385 validation_loss: 0.5209885951878672\n",
      "epochs: 71 trainingloss: 0.2829673424714828 validation_loss: 0.520996312464835\n",
      "epochs: 72 trainingloss: 0.2829398667198806 validation_loss: 0.5210038265002771\n",
      "epochs: 73 trainingloss: 0.2829143167882201 validation_loss: 0.5210111270922739\n",
      "epochs: 74 trainingloss: 0.2828905502323897 validation_loss: 0.5210182067852742\n",
      "epochs: 75 trainingloss: 0.2828684358061158 validation_loss: 0.5210250604825795\n",
      "epochs: 76 trainingloss: 0.2828478525191127 validation_loss: 0.5210316851072443\n",
      "epochs: 77 trainingloss: 0.28282868878045564 validation_loss: 0.5210380793054419\n",
      "epochs: 78 trainingloss: 0.2828108416188479 validation_loss: 0.5210442431871026\n",
      "epochs: 79 trainingloss: 0.2827942159723558 validation_loss: 0.5210501780992643\n",
      "epochs: 80 trainingloss: 0.2830689934091367 validation_loss: 0.5210558864281509\n",
      "epochs: 81 trainingloss: 0.2830545197468137 validation_loss: 0.52106137142649\n",
      "epochs: 82 trainingloss: 0.2830410260270668 validation_loss: 0.5210666370629974\n",
      "epochs: 83 trainingloss: 0.28302844272867894 validation_loss: 0.5210716878913438\n",
      "epochs: 84 trainingloss: 0.28301670553103087 validation_loss: 0.5210765289362408\n",
      "epochs: 85 trainingloss: 0.2830057549013543 validation_loss: 0.5210811655945726\n",
      "epochs: 86 trainingloss: 0.2829955357169446 validation_loss: 0.52108560354975\n",
      "epochs: 87 trainingloss: 0.2829859969191521 validation_loss: 0.5210898486976759\n",
      "epochs: 88 trainingloss: 0.2829770911962707 validation_loss: 0.5210939070829215\n",
      "epochs: 89 trainingloss: 0.2829687746927707 validation_loss: 0.5210977848438619\n",
      "epochs: 90 trainingloss: 0.2829610067425203 validation_loss: 0.5211014881656783\n",
      "epochs: 91 trainingloss: 0.2829537496238721 validation_loss: 0.5211050232402694\n",
      "epochs: 92 trainingloss: 0.2829469683347324 validation_loss: 0.5211083962322081\n",
      "epochs: 93 trainingloss: 0.28294063038584244 validation_loss: 0.5211116132500094\n",
      "epochs: 94 trainingloss: 0.28293470561071266 validation_loss: 0.5211146803220421\n",
      "epochs: 95 trainingloss: 0.2829291659907964 validation_loss: 0.5211176033764999\n",
      "epochs: 96 trainingloss: 0.282923985494574 validation_loss: 0.5211203882249233\n",
      "epochs: 97 trainingloss: 0.28291913992939505 validation_loss: 0.521123040548818\n",
      "epochs: 98 trainingloss: 0.2829146068049887 validation_loss: 0.5211255658889675\n",
      "epochs: 99 trainingloss: 0.2829103652076799 validation_loss: 0.5211279696370934\n",
      "epochs: 100 trainingloss: 0.28290639568440684 validation_loss: 0.5211302570295455\n",
      "epochs: 101 trainingloss: 0.28290268013574105 validation_loss: 0.5211324331427555\n",
      "epochs: 102 trainingloss: 0.2828992017171621 validation_loss: 0.52113450289021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 103 trainingloss: 0.28289594474791985 validation_loss: 0.521136471020725\n",
      "epochs: 104 trainingloss: 0.28289289462685907 validation_loss: 0.5211383421178489\n",
      "epochs: 105 trainingloss: 0.2828900377546541 validation_loss: 0.5211401206002131\n",
      "epochs: 106 trainingloss: 0.28288736146192583 validation_loss: 0.5211418107226973\n",
      "epochs: 107 trainingloss: 0.282884853942783 validation_loss: 0.521143416578278\n",
      "epochs: 108 trainingloss: 0.2828825041933498 validation_loss: 0.5211449421004475\n",
      "epochs: 109 trainingloss: 0.28288030195487834 validation_loss: 0.5211463910661107\n",
      "epochs: 110 trainingloss: 0.2828782376611103 validation_loss: 0.5211477670988678\n",
      "epochs: 111 trainingloss: 0.28287630238951567 validation_loss: 0.5211490736726124\n",
      "epochs: 112 trainingloss: 0.2828744878161429 validation_loss: 0.5211503141153823\n",
      "epochs: 113 trainingloss: 0.28287278617378114 validation_loss: 0.521151491613394\n",
      "epochs: 114 trainingloss: 0.28287119021318385 validation_loss: 0.5211526092152323\n",
      "epochs: 115 trainingloss: 0.28286969316710947 validation_loss: 0.5211536698361274\n",
      "epochs: 116 trainingloss: 0.2828682887169864 validation_loss: 0.5211546762623075\n",
      "epochs: 117 trainingloss: 0.28286697096197455 validation_loss: 0.5211556311553733\n",
      "epochs: 118 trainingloss: 0.2828657343902585 validation_loss: 0.521156537056683\n",
      "epochs: 119 trainingloss: 0.2828645738524058 validation_loss: 0.5211573963917167\n",
      "epochs: 120 trainingloss: 0.28286348453661925 validation_loss: 0.5211582114744012\n",
      "epochs: 121 trainingloss: 0.28286246194575865 validation_loss: 0.5211589845113808\n",
      "epochs: 122 trainingloss: 0.282861501875993 validation_loss: 0.5211597176062189\n",
      "epochs: 123 trainingloss: 0.282860600396955 validation_loss: 0.5211604127635199\n",
      "epochs: 124 trainingloss: 0.2828597538333014 validation_loss: 0.521161071892959\n",
      "epochs: 125 trainingloss: 0.2828589587475627 validation_loss: 0.521161696813215\n",
      "epochs: 126 trainingloss: 0.28285821192419947 validation_loss: 0.5211622892558034\n",
      "epochs: 127 trainingloss: 0.2828575103547606 validation_loss: 0.521162850868794\n",
      "epochs: 128 trainingloss: 0.28285685122408 validation_loss: 0.5211633832204252\n",
      "epochs: 129 trainingloss: 0.2828562318974294 validation_loss: 0.5211638878025993\n",
      "epochs: 130 trainingloss: 0.28285564990855566 validation_loss: 0.5211643660342622\n",
      "epochs: 131 trainingloss: 0.28285510294854305 validation_loss: 0.5211648192646746\n",
      "epochs: 132 trainingloss: 0.2828545888554377 validation_loss: 0.5211652487765563\n",
      "epochs: 133 trainingloss: 0.2828541056045872 validation_loss: 0.521165655789127\n",
      "epochs: 134 trainingloss: 0.28285365129963047 validation_loss: 0.5211660414610236\n",
      "epochs: 135 trainingloss: 0.2828532241641119 validation_loss: 0.5211664068931138\n",
      "epochs: 136 trainingloss: 0.28285282253364574 validation_loss: 0.5211667531311923\n",
      "epochs: 137 trainingloss: 0.282852444848636 validation_loss: 0.5211670811685725\n",
      "epochs: 138 trainingloss: 0.28285208964746067 validation_loss: 0.5211673919485736\n",
      "epochs: 139 trainingloss: 0.28285175556013403 validation_loss: 0.5211676863668993\n",
      "epochs: 140 trainingloss: 0.2828514413023791 validation_loss: 0.5211679652739224\n",
      "epochs: 141 trainingloss: 0.28285114567010883 validation_loss: 0.521168229476869\n",
      "epochs: 142 trainingloss: 0.2828508675342652 validation_loss: 0.5211684797419063\n",
      "epochs: 143 trainingloss: 0.28285060583600485 validation_loss: 0.5211687167961426\n",
      "epochs: 144 trainingloss: 0.28285035958220955 validation_loss: 0.5211689413295357\n",
      "epochs: 145 trainingloss: 0.2828501278412824 validation_loss: 0.5211691539967186\n",
      "epochs: 146 trainingloss: 0.28284990973922697 validation_loss: 0.5211693554187384\n",
      "epochs: 147 trainingloss: 0.2828497044559931 validation_loss: 0.5211695461847229\n",
      "epochs: 148 trainingloss: 0.2828495112220441 validation_loss: 0.5211697268534607\n",
      "epochs: 149 trainingloss: 0.28284932931516205 validation_loss: 0.5211698979549181\n",
      "epochs: 150 trainingloss: 0.28284915805746097 validation_loss: 0.5211700599916786\n",
      "epochs: 151 trainingloss: 0.28284899681258685 validation_loss: 0.5211702134403176\n",
      "epochs: 152 trainingloss: 0.2828488449831076 validation_loss: 0.5211703587527098\n",
      "epochs: 153 trainingloss: 0.282848702008069 validation_loss: 0.5211704963572765\n",
      "epochs: 154 trainingloss: 0.28284856736070735 validation_loss: 0.5211706266601738\n",
      "epochs: 155 trainingloss: 0.2828484405463145 validation_loss: 0.5211707500464184\n",
      "epochs: 156 trainingloss: 0.28284832110022834 validation_loss: 0.5211708668809665\n",
      "epochs: 157 trainingloss: 0.28284820858597376 validation_loss: 0.5211709775097338\n",
      "epochs: 158 trainingloss: 0.2828481025934991 validation_loss: 0.5211710822605689\n",
      "epochs: 159 trainingloss: 0.2828480027375411 validation_loss: 0.521171181444179\n",
      "epochs: 160 trainingloss: 0.28284790865608944 validation_loss: 0.5211712753550077\n",
      "epochs: 161 trainingloss: 0.28284782000894815 validation_loss: 0.5211713642720713\n",
      "epochs: 162 trainingloss: 0.28284773647638817 validation_loss: 0.5211714484597539\n",
      "epochs: 163 trainingloss: 0.28284765775788817 validation_loss: 0.5211715281685626\n",
      "epochs: 164 trainingloss: 0.28284758357095363 validation_loss: 0.5211716036358404\n",
      "epochs: 165 trainingloss: 0.28284751365000504 validation_loss: 0.5211716750864538\n",
      "epochs: 166 trainingloss: 0.2828474477453513 validation_loss: 0.5211717427334333\n",
      "epochs: 167 trainingloss: 0.2828473856222056 validation_loss: 0.5211718067785914\n",
      "epochs: 168 trainingloss: 0.2828473270597845 validation_loss: 0.5211718674131053\n",
      "epochs: 169 trainingloss: 0.28284727185044944 validation_loss: 0.5211719248180683\n",
      "epochs: 170 trainingloss: 0.2828472197989079 validation_loss: 0.5211719791650178\n",
      "epochs: 171 trainingloss: 0.2828471707214637 validation_loss: 0.5211720306164311\n",
      "epochs: 172 trainingloss: 0.28284712444531157 validation_loss: 0.5211720793262025\n",
      "epochs: 173 trainingloss: 0.2828470808078791 validation_loss: 0.5211721254400863\n",
      "epochs: 174 trainingloss: 0.2828470396562038 validation_loss: 0.5211721690961301\n",
      "epochs: 175 trainingloss: 0.2828470008463617 validation_loss: 0.5211722104250721\n",
      "epochs: 176 trainingloss: 0.2828469642429125 validation_loss: 0.5211722495507282\n",
      "epochs: 177 trainingloss: 0.282846929718394 validation_loss: 0.5211722865903551\n",
      "epochs: 178 trainingloss: 0.28284689715284217 validation_loss: 0.5211723216549922\n",
      "epochs: 179 trainingloss: 0.2828468664333385 validation_loss: 0.521172354849793\n",
      "epochs: 180 trainingloss: 0.28284683745359007 validation_loss: 0.5211723862743308\n",
      "epochs: 181 trainingloss: 0.28284681011353463 validation_loss: 0.5211724160228948\n",
      "epochs: 182 trainingloss: 0.28284678431896365 validation_loss: 0.5211724441847686\n",
      "epochs: 183 trainingloss: 0.2828467599811751 validation_loss: 0.5211724708444927\n",
      "epochs: 184 trainingloss: 0.28284673701664764 validation_loss: 0.5211724960821168\n",
      "epochs: 185 trainingloss: 0.2828467153467288 validation_loss: 0.5211725199734353\n",
      "epochs: 186 trainingloss: 0.2828466948973472 validation_loss: 0.5211725425902135\n",
      "epochs: 187 trainingloss: 0.2828466755987395 validation_loss: 0.5211725640004009\n",
      "epochs: 188 trainingloss: 0.28284665738519826 validation_loss: 0.5211725842683314\n",
      "epochs: 189 trainingloss: 0.28284664019482925 validation_loss: 0.5211726034549155\n",
      "epochs: 190 trainingloss: 0.28284662396932453 validation_loss: 0.5211726216178224\n",
      "epochs: 191 trainingloss: 0.2828466086537529 validation_loss: 0.5211726388116507\n",
      "epochs: 192 trainingloss: 0.2828465941963636 validation_loss: 0.5211726550880927\n",
      "epochs: 193 trainingloss: 0.28284658054839334 validation_loss: 0.5211726704960858\n",
      "epochs: 194 trainingloss: 0.2828465676638929 validation_loss: 0.5211726850819622\n",
      "epochs: 195 trainingloss: 0.2828465554995632 validation_loss: 0.5211726988895847\n",
      "epochs: 196 trainingloss: 0.28284654401460024 validation_loss: 0.5211727119604785\n",
      "epochs: 197 trainingloss: 0.2828465331705441 validation_loss: 0.5211727243339551\n",
      "epochs: 198 trainingloss: 0.28284652293114815 validation_loss: 0.5211727360472319\n",
      "epochs: 199 trainingloss: 0.2828465132622462 validation_loss: 0.5211727471355396\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(train2_vector, Y2_tr, \n",
    "                                             X_dev=dev2_vector, \n",
    "                                             Y_dev=Y2_dev,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.01, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:31:09.903453Z",
     "start_time": "2020-02-15T14:31:09.901360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x344 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation process \n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 4.3), dpi=80)\n",
    "plt.title(\"Training Monitoring\")\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loss_tr_count,color='blue',linewidth=3.0)\n",
    "plt.plot(dev_loss_count,color='orange',linewidth=3.0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_te=[]\n",
    "for i in range(0,len(test2_list)):\n",
    "     Y_te.append(test2_list[i][0])\n",
    "\n",
    "preds_te=predict_class(test2_vector,w_count)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:32:12.606498Z",
     "start_time": "2020-02-15T14:32:12.604164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8655555555555555\n",
      "Precision: 0.8678678463011938\n",
      "Recall: 0.8655555555555555\n",
      "F1-Score: 0.8652497905147577\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5000)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:32:26.224693Z",
     "start_time": "2020-02-15T14:32:26.221886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:World-positive: ['cover', 'captures', 'fancy', 'deeper', 'bus', 'williams', 'camera', 'process', 'will', 'learned']\n",
      "class:Sports-positive: ['going', 'obvious', 'bastard', 'vehicle', 'justice', 'convincingly', 'sports', 'blues', 'hammer', 'paranoid']\n",
      "class:Business-positive: [('but', 'if'), 'quest', 'older', 'saturday', 'cartoonish', 'gwyneth', 'scene', 'others', 'screaming', 'fincher']\n",
      "class:World-negative: ['holiday', 'vehicle', ('but', 'if'), 'quest', 'justice', 'listen', 'melodrama', 'older', 'sports', 'saturday']\n",
      "class:Sports-negative: ['camera', ('but', 'if'), 'captures', 'straight', 'holding', ('but', 'end'), ('not', 'mention'), ('end', 'film'), 'cover', 'deserved']\n",
      "class:Business-negative: ['going', 'cover', 'obvious', 'bastard', 'learned', 'paranoid', 'ever', 'lake', 'gordon', 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Print the top-10 words for each class respectively.\n",
    "\n",
    "positive_list_World,negative_list_World=top_words(w_count[0].tolist())\n",
    "positive_list_Sports,negative_list_Sports=top_words(w_count[1].tolist())\n",
    "positive_list_Business,negative_list_Business=top_words(w_count[2].tolist())\n",
    "print('class:World-positive:',positive_list_World)\n",
    "print('class:Sports-positive:',positive_list_Sports)\n",
    "print('class:Business-positive:',positive_list_Business)\n",
    "print('class:World-negative:',negative_list_World)\n",
    "print('class:Sports-negative:',negative_list_Sports)\n",
    "print('class:Business-negative:',negative_list_Business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:19.856538Z",
     "start_time": "2020-02-15T14:16:19.852547Z"
    }
   },
   "source": [
    "1  learning rate is 0.01,because at this time, the curve drops most smoothly,But when the epochs is more than 20, the change of loss becomes very small.Regularisation strength is 2*weights*lr*alpha.This regularisation can mprove the precision of the model.\n",
    "2 The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
    "3 In my experiment, regularisation strength has little effect on the experimental results, but it does improve the precision of the model.This is because regularization avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate BOW-tfidf..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:42.567569Z",
     "start_time": "2020-02-15T14:16:42.562560Z"
    }
   },
   "source": [
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |  0.867 | 0.865  | 0.865  |\n",
    "| BOW-tfidf  | 0.861  | 0.861  |  0.860 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_train2, df_train2, ngram_counts_train2 = get_vocab(X_tr_raw2, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "\n",
    "vocab_dev2, df_dev2, ngram_counts_train2 = get_vocab(X_dev_raw2, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "\n",
    "vocab_test2, df_test2, ngram_counts_train2 = get_vocab(X_test_raw2, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_vector=vectorise_tfidf(train2_list_ngram, vocab_train2,df_train2)\n",
    "dev2_vector=vectorise_tfidf(development2_ngram, vocab_train2,df_dev2)\n",
    "test2_vector=vectorise_tfidf(test2_ngram, vocab_train2,df_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 trainingloss: 0.39005644823112545 validation_loss: 0.8519649570469787\n",
      "epochs: 1 trainingloss: 0.2917781828249325 validation_loss: 0.746619006001304\n",
      "epochs: 2 trainingloss: 0.23710885481020477 validation_loss: 0.6799390253768253\n",
      "epochs: 3 trainingloss: 0.20574031999521844 validation_loss: 0.6284960306048246\n",
      "epochs: 4 trainingloss: 0.18286776647589045 validation_loss: 0.6031630947694815\n",
      "epochs: 5 trainingloss: 0.16924828267413505 validation_loss: 0.5859120555147439\n",
      "epochs: 6 trainingloss: 0.15654961101233986 validation_loss: 0.5736894335872155\n",
      "epochs: 7 trainingloss: 0.14819519681879142 validation_loss: 0.564810932508113\n",
      "epochs: 8 trainingloss: 0.1411014624692319 validation_loss: 0.5582598118863957\n",
      "epochs: 9 trainingloss: 0.13623118335255116 validation_loss: 0.5533819347126507\n",
      "epochs: 10 trainingloss: 0.1315426294445174 validation_loss: 0.5497360258916857\n",
      "epochs: 11 trainingloss: 0.1292785333660281 validation_loss: 0.5470131781175466\n",
      "epochs: 12 trainingloss: 0.1261127091564553 validation_loss: 0.5449907401117627\n",
      "epochs: 13 trainingloss: 0.12406501424877678 validation_loss: 0.5435045376451462\n",
      "epochs: 14 trainingloss: 0.12216586934500898 validation_loss: 0.5369755147084065\n",
      "epochs: 15 trainingloss: 0.12032161825775275 validation_loss: 0.5361961097318635\n",
      "epochs: 16 trainingloss: 0.11877061289882902 validation_loss: 0.5356658159579577\n",
      "epochs: 17 trainingloss: 0.11745966391522017 validation_loss: 0.5353286656335683\n",
      "epochs: 18 trainingloss: 0.11634660216452503 validation_loss: 0.5351409126246921\n",
      "epochs: 19 trainingloss: 0.11539766628339583 validation_loss: 0.5350681955686567\n",
      "epochs: 20 trainingloss: 0.11458559483907178 validation_loss: 0.5350834294529608\n",
      "epochs: 21 trainingloss: 0.11388821005812036 validation_loss: 0.5298052850933327\n",
      "epochs: 22 trainingloss: 0.11328735089351373 validation_loss: 0.5299079490642999\n",
      "epochs: 23 trainingloss: 0.11276805861804538 validation_loss: 0.5300495864043652\n",
      "epochs: 24 trainingloss: 0.11231794783730495 validation_loss: 0.5302193340703804\n",
      "epochs: 25 trainingloss: 0.11192671560044208 validation_loss: 0.5304085988774758\n",
      "epochs: 26 trainingloss: 0.11158575470143368 validation_loss: 0.5306106032880946\n",
      "epochs: 27 trainingloss: 0.1112878465138678 validation_loss: 0.5308200246837742\n",
      "epochs: 28 trainingloss: 0.11102691518314195 validation_loss: 0.531032707592482\n",
      "epochs: 29 trainingloss: 0.11079782960837611 validation_loss: 0.5312454332677321\n",
      "epochs: 30 trainingloss: 0.11059624296897005 validation_loss: 0.531455734634915\n",
      "epochs: 31 trainingloss: 0.11041846197746125 validation_loss: 0.5316617473153715\n",
      "epochs: 32 trainingloss: 0.11026133983421879 validation_loss: 0.5318620894698968\n",
      "epochs: 33 trainingloss: 0.1101221882005593 validation_loss: 0.5320557647508734\n",
      "epochs: 34 trainingloss: 0.10999870451990934 validation_loss: 0.5322420838428792\n",
      "epochs: 35 trainingloss: 0.10988891178937567 validation_loss: 0.526860815117974\n",
      "epochs: 36 trainingloss: 0.10979110847878326 validation_loss: 0.5270195013851281\n",
      "epochs: 37 trainingloss: 0.10970382675575867 validation_loss: 0.5271708555228508\n",
      "epochs: 38 trainingloss: 0.10962579753631012 validation_loss: 0.5273148334899335\n",
      "epochs: 39 trainingloss: 0.10955592116449789 validation_loss: 0.5274514851773986\n",
      "epochs: 40 trainingloss: 0.10978214967174767 validation_loss: 0.5275809323373383\n",
      "epochs: 41 trainingloss: 0.10972547359340261 validation_loss: 0.5277033507822657\n",
      "epochs: 42 trainingloss: 0.10967447987829101 validation_loss: 0.5278189560621227\n",
      "epochs: 43 trainingloss: 0.1096285307739379 validation_loss: 0.5279279919741051\n",
      "epochs: 44 trainingloss: 0.10958706798879149 validation_loss: 0.5100330540347279\n",
      "epochs: 45 trainingloss: 0.10954960184915288 validation_loss: 0.5101142502918752\n",
      "epochs: 46 trainingloss: 0.10951570205418652 validation_loss: 0.5101908092671903\n",
      "epochs: 47 trainingloss: 0.10948498977738372 validation_loss: 0.5102629261364827\n",
      "epochs: 48 trainingloss: 0.10945713090491756 validation_loss: 0.5103308014877654\n",
      "epochs: 49 trainingloss: 0.10943183023591707 validation_loss: 0.5103946379093238\n",
      "epochs: 50 trainingloss: 0.10940882649827278 validation_loss: 0.5104546373159858\n",
      "epochs: 51 trainingloss: 0.10973983107439894 validation_loss: 0.5105109988776777\n",
      "epochs: 52 trainingloss: 0.10972036597144931 validation_loss: 0.5105639174384309\n",
      "epochs: 53 trainingloss: 0.10970260795940107 validation_loss: 0.5106135823338006\n",
      "epochs: 54 trainingloss: 0.10968639150518873 validation_loss: 0.5106601765309662\n",
      "epochs: 55 trainingloss: 0.10967156886373976 validation_loss: 0.5107038760292261\n",
      "epochs: 56 trainingloss: 0.10965800797009043 validation_loss: 0.5107448494696571\n",
      "epochs: 57 trainingloss: 0.10964559060412618 validation_loss: 0.5107832579118725\n",
      "epochs: 58 trainingloss: 0.10963421078994287 validation_loss: 0.5108192547433368\n",
      "epochs: 59 trainingloss: 0.10962377339746063 validation_loss: 0.5108529856929258\n",
      "epochs: 60 trainingloss: 0.10961419291868954 validation_loss: 0.5108845889255678\n",
      "epochs: 61 trainingloss: 0.10960539239506716 validation_loss: 0.5109141951990372\n",
      "epochs: 62 trainingloss: 0.1095973024756848 validation_loss: 0.5109419280674896\n",
      "epochs: 63 trainingloss: 0.10958986058910979 validation_loss: 0.5109679041192107\n",
      "epochs: 64 trainingloss: 0.10958301021395005 validation_loss: 0.5109922332384425\n",
      "epochs: 65 trainingloss: 0.10957670023539116 validation_loss: 0.5110150188831265\n",
      "epochs: 66 trainingloss: 0.10957088437670862 validation_loss: 0.5110363583720157\n",
      "epochs: 67 trainingloss: 0.10956552069626618 validation_loss: 0.5110563431759501\n",
      "epochs: 68 trainingloss: 0.10956057114180098 validation_loss: 0.5110750592091868\n",
      "epochs: 69 trainingloss: 0.10955600115490209 validation_loss: 0.5110925871175743\n",
      "epochs: 70 trainingloss: 0.10955177931953602 validation_loss: 0.511109002561104\n",
      "epochs: 71 trainingloss: 0.10954787704927528 validation_loss: 0.5111243764889699\n",
      "epochs: 72 trainingloss: 0.10954426830859686 validation_loss: 0.5111387754057568\n",
      "epochs: 73 trainingloss: 0.10954092936420741 validation_loss: 0.5111522616277872\n",
      "epochs: 74 trainingloss: 0.1095378385628724 validation_loss: 0.5111648935289614\n",
      "epochs: 75 trainingloss: 0.10953497613267117 validation_loss: 0.5111767257757023\n",
      "epochs: 76 trainingloss: 0.10953232400499274 validation_loss: 0.5111878095508025\n",
      "epochs: 77 trainingloss: 0.10952986565491041 validation_loss: 0.5111981927661623\n",
      "epochs: 78 trainingloss: 0.10952758595787047 validation_loss: 0.5112079202645028\n",
      "epochs: 79 trainingloss: 0.10952547106088115 validation_loss: 0.5112170340102654\n",
      "epochs: 80 trainingloss: 0.10952350826660326 validation_loss: 0.5112255732699656\n",
      "epochs: 81 trainingloss: 0.10952168592893695 validation_loss: 0.5112335747823372\n",
      "epochs: 82 trainingloss: 0.10951999335886688 validation_loss: 0.511241072918635\n",
      "epochs: 83 trainingloss: 0.10951842073946275 validation_loss: 0.5112480998334898\n",
      "epochs: 84 trainingloss: 0.10951695904907786 validation_loss: 0.5112546856067446\n",
      "epochs: 85 trainingloss: 0.1095155999918786 validation_loss: 0.5112608583766751\n",
      "epochs: 86 trainingloss: 0.10951433593494872 validation_loss: 0.5112666444650408\n",
      "epochs: 87 trainingloss: 0.10951315985129395 validation_loss: 0.511272068494373\n",
      "epochs: 88 trainingloss: 0.10951206526814278 validation_loss: 0.5112771534979318\n",
      "epochs: 89 trainingloss: 0.10951104622001576 validation_loss: 0.5112819210227172\n",
      "epochs: 90 trainingloss: 0.1095100972060822 validation_loss: 0.5112863912259497\n",
      "epochs: 91 trainingloss: 0.10950921315138314 validation_loss: 0.5112905829653833\n",
      "epochs: 92 trainingloss: 0.10950838937153815 validation_loss: 0.5112945138838242\n",
      "epochs: 93 trainingloss: 0.10950762154060567 validation_loss: 0.5112982004881991\n",
      "epochs: 94 trainingloss: 0.10950690566178004 validation_loss: 0.5113016582235074\n",
      "epochs: 95 trainingloss: 0.10950623804066673 validation_loss: 0.5113049015419731\n",
      "epochs: 96 trainingloss: 0.10950561526088617 validation_loss: 0.5113079439676934\n",
      "epochs: 97 trainingloss: 0.10950503416178617 validation_loss: 0.5113107981570741\n",
      "epochs: 98 trainingloss: 0.10950449181806893 validation_loss: 0.5113134759553073\n",
      "epochs: 99 trainingloss: 0.10950398552115724 validation_loss: 0.511315988449158\n",
      "epochs: 100 trainingloss: 0.10950351276213621 validation_loss: 0.5113183460162883\n",
      "epochs: 101 trainingloss: 0.10950307121613038 validation_loss: 0.5113205583713455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 102 trainingloss: 0.10950265872798605 validation_loss: 0.5113226346090253\n",
      "epochs: 103 trainingloss: 0.10950227329914078 validation_loss: 0.5113245832443059\n",
      "epochs: 104 trainingloss: 0.10950191307557686 validation_loss: 0.5113264122500376\n",
      "epochs: 105 trainingloss: 0.10950157633676184 validation_loss: 0.511328129092066\n",
      "epochs: 106 trainingloss: 0.10950126148548603 validation_loss: 0.5113297407620429\n",
      "epochs: 107 trainingloss: 0.10950096703852717 validation_loss: 0.511331253808087\n",
      "epochs: 108 trainingloss: 0.10950069161806229 validation_loss: 0.5113326743634289\n",
      "epochs: 109 trainingloss: 0.10950043394376832 validation_loss: 0.5113340081731755\n",
      "epochs: 110 trainingloss: 0.10950019282555004 validation_loss: 0.511335260619322\n",
      "epochs: 111 trainingloss: 0.10949996715684193 validation_loss: 0.5113364367441219\n",
      "epochs: 112 trainingloss: 0.10949975590843797 validation_loss: 0.5113375412719275\n",
      "epochs: 113 trainingloss: 0.10949955812280263 validation_loss: 0.5113385786296034\n",
      "epochs: 114 trainingloss: 0.1094993729088265 validation_loss: 0.5113395529656025\n",
      "epochs: 115 trainingloss: 0.1094991994369834 validation_loss: 0.5113404681677954\n",
      "epochs: 116 trainingloss: 0.10949903693486476 validation_loss: 0.5113413278801444\n",
      "epochs: 117 trainingloss: 0.10949888468305283 validation_loss: 0.5113421355182807\n",
      "epochs: 118 trainingloss: 0.10949874201130827 validation_loss: 0.5113428942840755\n",
      "epochs: 119 trainingloss: 0.10949860829504957 validation_loss: 0.5113436071792591\n",
      "epochs: 120 trainingloss: 0.10949848295209524 validation_loss: 0.5113442770181578\n",
      "epochs: 121 trainingloss: 0.10949836543964957 validation_loss: 0.511344906439607\n",
      "epochs: 122 trainingloss: 0.10949825525151935 validation_loss: 0.5113454979180864\n",
      "epochs: 123 trainingloss: 0.1094981519155308 validation_loss: 0.511346053774142\n",
      "epochs: 124 trainingloss: 0.10949805499114088 validation_loss: 0.5113465761841304\n",
      "epochs: 125 trainingloss: 0.10949796406722627 validation_loss: 0.5113470671893343\n",
      "epochs: 126 trainingloss: 0.10949787876003171 validation_loss: 0.5113475287044951\n",
      "epochs: 127 trainingloss: 0.10949779871126993 validation_loss: 0.5113479625257905\n",
      "epochs: 128 trainingloss: 0.10949772358635902 validation_loss: 0.5113483703383049\n",
      "epochs: 129 trainingloss: 0.1094976530727891 validation_loss: 0.5113487537230158\n",
      "epochs: 130 trainingloss: 0.10949758687860495 validation_loss: 0.5113491141633371\n",
      "epochs: 131 trainingloss: 0.10949752473099786 validation_loss: 0.5113494530512372\n",
      "epochs: 132 trainingloss: 0.1094974663750011 validation_loss: 0.5113497716929752\n",
      "epochs: 133 trainingloss: 0.10949741157227483 validation_loss: 0.5113500713144629\n",
      "epochs: 134 trainingloss: 0.10949736009997864 validation_loss: 0.5113503530662866\n",
      "epochs: 135 trainingloss: 0.10949731174972593 validation_loss: 0.5113506180284149\n",
      "epochs: 136 trainingloss: 0.10949726632660862 validation_loss: 0.5113508672145988\n",
      "epochs: 137 trainingloss: 0.10949722364829223 validation_loss: 0.5113511015765003\n",
      "epochs: 138 trainingloss: 0.10949718354417524 validation_loss: 0.5113513220075522\n",
      "epochs: 139 trainingloss: 0.10949714585460474 validation_loss: 0.5113515293465788\n",
      "epochs: 140 trainingloss: 0.10949711043014884 validation_loss: 0.5113517243811873\n",
      "epochs: 141 trainingloss: 0.10949707713091719 validation_loss: 0.5113519078509392\n",
      "epochs: 142 trainingloss: 0.1094970458259318 validation_loss: 0.5113520804503306\n",
      "epochs: 143 trainingloss: 0.10949701639253734 validation_loss: 0.5113522428315771\n",
      "epochs: 144 trainingloss: 0.10949698871585352 validation_loss: 0.5113523956072282\n",
      "epochs: 145 trainingloss: 0.10949696268826803 validation_loss: 0.511352539352615\n",
      "epochs: 146 trainingloss: 0.10949693820895594 validation_loss: 0.5113526746081454\n",
      "epochs: 147 trainingloss: 0.10949691518344257 validation_loss: 0.5113528018814546\n",
      "epochs: 148 trainingloss: 0.1094968935231846 validation_loss: 0.5113529216494234\n",
      "epochs: 149 trainingloss: 0.10949687314519098 validation_loss: 0.5113530343600657\n",
      "epochs: 150 trainingloss: 0.1094968539716581 validation_loss: 0.5113531404343027\n",
      "epochs: 151 trainingloss: 0.1094968359296384 validation_loss: 0.5113532402676245\n",
      "epochs: 152 trainingloss: 0.1094968189507267 validation_loss: 0.5113533342316492\n",
      "epochs: 153 trainingloss: 0.10949680297076786 validation_loss: 0.51135342267558\n",
      "epochs: 154 trainingloss: 0.10949678792958402 validation_loss: 0.5113535059275814\n",
      "epochs: 155 trainingloss: 0.10949677377072241 validation_loss: 0.5113535842960606\n",
      "epochs: 156 trainingloss: 0.10949676044121484 validation_loss: 0.5113536580708744\n",
      "epochs: 157 trainingloss: 0.10949674789135934 validation_loss: 0.5113537275244606\n",
      "epochs: 158 trainingloss: 0.10949673607450922 validation_loss: 0.5113537929128974\n",
      "epochs: 159 trainingloss: 0.10949672494688142 validation_loss: 0.5113538544769016\n",
      "epochs: 160 trainingloss: 0.1094967144673773 validation_loss: 0.5113539124427584\n",
      "epochs: 161 trainingloss: 0.10949670459740943 validation_loss: 0.5113539670232022\n",
      "epochs: 162 trainingloss: 0.10949669530074646 validation_loss: 0.511354018418237\n",
      "epochs: 163 trainingloss: 0.10949668654336465 validation_loss: 0.5113540668159068\n",
      "epochs: 164 trainingloss: 0.10949667829330885 validation_loss: 0.5113541123930213\n",
      "epochs: 165 trainingloss: 0.1094966705205636 validation_loss: 0.511354155315836\n",
      "epochs: 166 trainingloss: 0.10949666319693284 validation_loss: 0.5113541957406895\n",
      "epochs: 167 trainingloss: 0.10949665629592568 validation_loss: 0.5113542338146034\n",
      "epochs: 168 trainingloss: 0.10949664979265056 validation_loss: 0.5113542696758437\n",
      "epochs: 169 trainingloss: 0.10949664366371716 validation_loss: 0.5113543034544492\n",
      "epochs: 170 trainingloss: 0.10949663788714112 validation_loss: 0.511354335272726\n",
      "epochs: 171 trainingloss: 0.10949663244226064 validation_loss: 0.511354365245715\n",
      "epochs: 172 trainingloss: 0.10949662730965173 validation_loss: 0.5113543934816275\n",
      "epochs: 173 trainingloss: 0.10949662247105396 validation_loss: 0.5113544200822562\n",
      "epochs: 174 trainingloss: 0.10949661790929881 validation_loss: 0.5113544451433594\n",
      "epochs: 175 trainingloss: 0.10949661360824155 validation_loss: 0.5113544687550243\n",
      "epochs: 176 trainingloss: 0.10949660955270044 validation_loss: 0.5113544910020067\n",
      "epochs: 177 trainingloss: 0.10949660572839641 validation_loss: 0.5113545119640496\n",
      "epochs: 178 trainingloss: 0.1094966021218981 validation_loss: 0.5113545317161832\n",
      "epochs: 179 trainingloss: 0.109496598720573 validation_loss: 0.5113545503290081\n",
      "epochs: 180 trainingloss: 0.10949659551253399 validation_loss: 0.5113545678689586\n",
      "epochs: 181 trainingloss: 0.10949659248659861 validation_loss: 0.5113545843985519\n",
      "epochs: 182 trainingloss: 0.10949658963224396 validation_loss: 0.5113545999766207\n",
      "epochs: 183 trainingloss: 0.10949658693956815 validation_loss: 0.5113546146585355\n",
      "epochs: 184 trainingloss: 0.10949658439925154 validation_loss: 0.5113546284964076\n",
      "epochs: 185 trainingloss: 0.10949658200252303 validation_loss: 0.511354641539285\n",
      "epochs: 186 trainingloss: 0.10949657974112652 validation_loss: 0.5113546538333338\n",
      "epochs: 187 trainingloss: 0.1094965776072892 validation_loss: 0.51135466542201\n",
      "epochs: 188 trainingloss: 0.10949657559369401 validation_loss: 0.5113546763462206\n",
      "epochs: 189 trainingloss: 0.10949657369345153 validation_loss: 0.5113546866444735\n",
      "epochs: 190 trainingloss: 0.10949657190007495 validation_loss: 0.5113546963530212\n",
      "epochs: 191 trainingloss: 0.10949657020745455 validation_loss: 0.5113547055059936\n",
      "epochs: 192 trainingloss: 0.10949656860983759 validation_loss: 0.5113547141355239\n",
      "epochs: 193 trainingloss: 0.10949656710180505 validation_loss: 0.5113547222718667\n",
      "epochs: 194 trainingloss: 0.10949656567825414 validation_loss: 0.5113547299435093\n",
      "epochs: 195 trainingloss: 0.10949656433437616 validation_loss: 0.5113547371772745\n",
      "epochs: 196 trainingloss: 0.10949656306564122 validation_loss: 0.5113547439984227\n",
      "epochs: 197 trainingloss: 0.10949656186778323 validation_loss: 0.5113547504307389\n",
      "epochs: 198 trainingloss: 0.10949656073678159 validation_loss: 0.5113547564966242\n",
      "epochs: 199 trainingloss: 0.10949655966884703 validation_loss: 0.5113547622171745\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(train2_vector, Y2_tr, \n",
    "                                             X_dev=dev2_vector, \n",
    "                                             Y_dev=Y2_dev,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.01, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_te=[]\n",
    "for i in range(0,len(test2_list)):\n",
    "     Y_te.append(test2_list[i][0])\n",
    "\n",
    "preds_te=predict_class(test2_vector,w_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8611111111111112\n",
      "Precision: 0.861561205905335\n",
      "Recall: 0.8611111111111112\n",
      "F1-Score: 0.8605779327317181\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
